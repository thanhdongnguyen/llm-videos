1
00:00:00,288 --> 00:00:02,373
Chris Anderson: Elon Musk,
great to see you.

2
00:00:02,373 --> 00:00:03,583
How are you?

3
00:00:03,583 --> 00:00:05,001
Elon Musk: Good. How are you?

4
00:00:05,001 --> 00:00:09,213
CA: We're here at the Texas Gigafactory
the day before this thing opens.

5
00:00:09,255 --> 00:00:11,007
It's been pretty crazy out there.

6
00:00:11,049 --> 00:00:14,343
Thank you so much
for making time on a busy day.

7
00:00:14,343 --> 00:00:18,056
I would love you to help us,
kind of, cast our minds,

8
00:00:18,097 --> 00:00:21,559
I don't know, 10, 20,
30 years into the future.

9
00:00:22,852 --> 00:00:26,314
And help us try to picture
what it would take

10
00:00:26,314 --> 00:00:29,400
to build a future that's worth
getting excited about.

11
00:00:29,442 --> 00:00:31,152
The last time you spoke at TED,

12
00:00:31,152 --> 00:00:33,946
you said that that was really
just a big driver.

13
00:00:33,988 --> 00:00:37,742
You know, you can talk about lots of other
reasons to do the work you're doing,

14
00:00:37,742 --> 00:00:41,829
but fundamentally, you want
to think about the future

15
00:00:41,871 --> 00:00:43,247
and not think that it sucks.

16
00:00:43,873 --> 00:00:45,041
EM: Yeah, absolutely.

17
00:00:45,083 --> 00:00:46,584
I think in general, you know,

18
00:00:46,626 --> 00:00:50,963
there's a lot of discussion of like,
this problem or that problem.

19
00:00:51,005 --> 00:00:54,383
And a lot of people are sad
about the future

20
00:00:54,425 --> 00:00:55,968
and they're ...

21
00:00:58,262 --> 00:00:59,430
Pessimistic.

22
00:00:59,472 --> 00:01:02,058
And I think ...

23
00:01:02,100 --> 00:01:03,476
this is ...

24
00:01:05,061 --> 00:01:06,229
This is not great.

25
00:01:06,270 --> 00:01:08,898
I mean, we really want
to wake up in the morning

26
00:01:08,940 --> 00:01:10,900
and look forward to the future.

27
00:01:10,942 --> 00:01:14,487
We want to be excited
about what's going to happen.

28
00:01:15,446 --> 00:01:20,326
And life cannot simply be about sort of,

29
00:01:20,368 --> 00:01:22,578
solving one miserable
problem after another.

30
00:01:22,578 --> 00:01:26,290
CA: So if you look forward 30 years,
you know, the year 2050

31
00:01:26,332 --> 00:01:29,585
has been labeled by scientists

32
00:01:29,627 --> 00:01:33,881
as this, kind of, almost like this
doomsday deadline on climate.

33
00:01:33,923 --> 00:01:37,343
There's a consensus of scientists,
a large consensus of scientists,

34
00:01:37,385 --> 00:01:42,682
who believe that if we haven't
completely eliminated greenhouse gases

35
00:01:42,723 --> 00:01:46,269
or offset them completely by 2050,

36
00:01:46,310 --> 00:01:49,272
effectively we're inviting
climate catastrophe.

37
00:01:49,272 --> 00:01:53,484
Do you believe there is a pathway
to avoid that catastrophe?

38
00:01:53,526 --> 00:01:55,111
And what would it look like?

39
00:01:56,904 --> 00:02:00,783
EM: Yeah, so I am not one
of the doomsday people,

40
00:02:00,783 --> 00:02:02,410
which may surprise you.

41
00:02:02,952 --> 00:02:05,454
I actually think we're on a good path.

42
00:02:07,123 --> 00:02:08,374
But at the same time,

43
00:02:08,416 --> 00:02:12,003
I want to caution against complacency.

44
00:02:12,003 --> 00:02:14,672
So, so long as we are not complacent,

45
00:02:14,714 --> 00:02:17,425
as long as we have a high sense of urgency

46
00:02:17,425 --> 00:02:21,470
about moving towards
a sustainable energy economy,

47
00:02:21,512 --> 00:02:23,222
then I think things will be fine.

48
00:02:25,433 --> 00:02:27,393
So I can't emphasize that enough,

49
00:02:27,435 --> 00:02:32,690
as long as we push hard
and are not complacent,

50
00:02:34,066 --> 00:02:35,651
the future is going to be great.

51
00:02:35,651 --> 00:02:36,777
Don't worry about it.

52
00:02:36,777 --> 00:02:37,945
I mean, worry about it,

53
00:02:37,987 --> 00:02:42,325
but if you worry about it, ironically,
it will be a self-unfulfilling prophecy.

54
00:02:42,867 --> 00:02:46,954
So, like, there are three elements
to a sustainable energy future.

55
00:02:47,371 --> 00:02:51,834
One is of sustainable energy generation,
which is primarily wind and solar.

56
00:02:51,876 --> 00:02:55,796
There's also hydro, geothermal,

57
00:02:55,838 --> 00:02:58,007
I'm actually pro-nuclear.

58
00:02:58,049 --> 00:03:00,176
I think nuclear is fine.

59
00:03:04,263 --> 00:03:07,225
But it's going to be primarily
solar and wind,

60
00:03:07,266 --> 00:03:11,604
as the primary generators of energy.

61
00:03:11,604 --> 00:03:15,441
The second part is you need batteries
to store the solar and wind energy

62
00:03:15,483 --> 00:03:17,735
because the sun
doesn't shine all the time,

63
00:03:17,777 --> 00:03:19,487
the wind doesn't blow all the time.

64
00:03:19,487 --> 00:03:22,198
So it's a lot of stationary battery packs.

65
00:03:23,032 --> 00:03:25,076
And then you need electric transport.

66
00:03:25,117 --> 00:03:27,870
So electric cars, electric planes, boats.

67
00:03:27,870 --> 00:03:29,330
And then ultimately,

68
00:03:30,373 --> 00:03:32,750
it’s not really possible
to make electric rockets,

69
00:03:32,792 --> 00:03:35,962
but you can make
the propellant used in rockets

70
00:03:36,003 --> 00:03:37,713
using sustainable energy.

71
00:03:37,755 --> 00:03:42,176
So ultimately, we can have a fully
sustainable energy economy.

72
00:03:42,593 --> 00:03:45,554
And it's those three things:

73
00:03:45,596 --> 00:03:48,474
solar/wind, stationary
battery pack, electric vehicles.

74
00:03:48,516 --> 00:03:51,978
So then what are the limiting
factors on progress?

75
00:03:51,978 --> 00:03:55,189
The limiting factor really will be
battery cell production.

76
00:03:55,606 --> 00:03:59,652
So that's going to really be
the fundamental rate driver.

77
00:03:59,694 --> 00:04:01,487
And then whatever the slowest element

78
00:04:01,487 --> 00:04:05,574
of the whole lithium-ion
battery cells supply chain,

79
00:04:05,616 --> 00:04:08,369
from mining and the many steps of refining

80
00:04:08,411 --> 00:04:11,122
to ultimately creating a battery cell

81
00:04:11,163 --> 00:04:12,581
and putting it into a pack,

82
00:04:12,623 --> 00:04:16,002
that will be the limiting factor
on progress towards sustainability.

83
00:04:16,043 --> 00:04:18,629
CA: All right, so we need to talk
more about batteries,

84
00:04:18,671 --> 00:04:20,923
because the key thing
that I want to understand,

85
00:04:20,923 --> 00:04:23,009
like, there seems to be
a scaling issue here

86
00:04:23,050 --> 00:04:25,886
that is kind of amazing and alarming.

87
00:04:25,886 --> 00:04:28,806
You have said that you have calculated

88
00:04:28,806 --> 00:04:34,478
that the amount of battery production
that the world needs for sustainability

89
00:04:34,478 --> 00:04:38,316
is 300 terawatt hours of batteries.

90
00:04:39,317 --> 00:04:40,484
That's the end goal?

91
00:04:40,484 --> 00:04:41,819
EM: Very rough numbers,

92
00:04:41,861 --> 00:04:45,072
and I certainly would invite others
to check our calculations

93
00:04:45,114 --> 00:04:47,700
because they may arrive
at different conclusions.

94
00:04:47,742 --> 00:04:53,831
But in order to transition, not just
current electricity production,

95
00:04:53,873 --> 00:04:57,877
but also heating and transport,

96
00:04:57,918 --> 00:05:01,714
which roughly triples the amount
of electricity that you need,

97
00:05:01,756 --> 00:05:06,802
it amounts to approximately 300 terawatt
hours of installed capacity.

98
00:05:06,802 --> 00:05:11,265
CA: So we need to give people
a sense of how big a task that is.

99
00:05:11,307 --> 00:05:13,517
I mean, here we are at the Gigafactory.

100
00:05:13,517 --> 00:05:18,606
You know, this is one of the biggest
buildings in the world.

101
00:05:19,565 --> 00:05:22,651
What I've read, and tell me
if this is still right,

102
00:05:22,693 --> 00:05:28,574
is that the goal here is to eventually
produce 100 gigawatt hours

103
00:05:28,616 --> 00:05:31,077
of batteries here a year eventually.

104
00:05:31,452 --> 00:05:33,329
EM: We will probably do more than that,

105
00:05:33,329 --> 00:05:37,458
but yes, hopefully we get there
within a couple of years.

106
00:05:37,500 --> 00:05:38,793
CA: Right.

107
00:05:38,793 --> 00:05:41,295
But I mean, that is one --

108
00:05:41,337 --> 00:05:43,172
EM: 0.1 terrawat hours.

109
00:05:43,172 --> 00:05:46,634
CA: But that's still 1/100
of what's needed.

110
00:05:46,675 --> 00:05:52,515
How much of the rest of that 100
is Tesla planning to take on

111
00:05:52,556 --> 00:05:57,978
let's say, between now and 2030, 2040,

112
00:05:58,020 --> 00:06:01,148
when we really need to see
the scale up happen?

113
00:06:01,774 --> 00:06:03,526
EM: I mean, these are just guesses.

114
00:06:03,567 --> 00:06:06,362
So please, people shouldn't
hold me to these things.

115
00:06:06,404 --> 00:06:08,572
It's not like this is like some --

116
00:06:08,614 --> 00:06:11,992
What tends to happen
is I'll make some like,

117
00:06:12,034 --> 00:06:13,285
you know, best guess

118
00:06:13,327 --> 00:06:15,371
and then people, in five years,

119
00:06:15,413 --> 00:06:17,540
there’ll be some jerk
that writes an article:

120
00:06:17,540 --> 00:06:20,418
"Elon said this would happen,
and it didn't happen.

121
00:06:20,459 --> 00:06:22,169
He's a liar and a fool."

122
00:06:22,211 --> 00:06:24,422
It's very annoying when that happens.

123
00:06:25,172 --> 00:06:27,800
So these are just guesses,
this is a conversation.

124
00:06:27,842 --> 00:06:28,968
CA: Right.

125
00:06:29,552 --> 00:06:34,014
EM: I think Tesla probably ends up
doing 10 percent of that.

126
00:06:35,099 --> 00:06:36,267
Roughly.

127
00:06:36,308 --> 00:06:37,601
CA: Let's say 2050

128
00:06:37,643 --> 00:06:43,524
we have this amazing, you know,
100 percent sustainable electric grid

129
00:06:43,566 --> 00:06:46,777
made up of, you know, some mixture
of the sustainable energy sources

130
00:06:46,777 --> 00:06:48,112
you talked about.

131
00:06:49,447 --> 00:06:52,283
That same grid probably
is offering the world

132
00:06:52,324 --> 00:06:54,201
really low-cost energy, isn't it,

133
00:06:54,243 --> 00:06:56,328
compared with now.

134
00:06:56,745 --> 00:06:59,874
And I'm curious about like,

135
00:06:59,915 --> 00:07:04,128
are people entitled to get
a little bit excited

136
00:07:04,170 --> 00:07:06,797
about the possibilities of that world?

137
00:07:06,839 --> 00:07:09,133
EM: People should be optimistic
about the future.

138
00:07:12,970 --> 00:07:15,723
Humanity will solve sustainable energy.

139
00:07:15,764 --> 00:07:20,311
It will happen if we, you know,
continue to push hard,

140
00:07:20,311 --> 00:07:24,815
the future is bright and good
from an energy standpoint.

141
00:07:25,983 --> 00:07:31,363
And then it will be possible to also use
that energy to do carbon sequestration.

142
00:07:31,363 --> 00:07:34,241
It takes a lot of energy to pull
carbon out of the atmosphere

143
00:07:34,283 --> 00:07:37,203
because in putting it in the atmosphere
it releases energy.

144
00:07:37,244 --> 00:07:39,705
So now, you know, obviously
in order to pull it out,

145
00:07:39,705 --> 00:07:41,290
you need to use a lot of energy.

146
00:07:41,332 --> 00:07:45,044
But if you've got a lot of sustainable
energy from wind and solar,

147
00:07:45,044 --> 00:07:46,712
you can actually sequester carbon.

148
00:07:46,712 --> 00:07:52,176
So you can reverse the CO2 parts
per million of the atmosphere and oceans.

149
00:07:52,593 --> 00:07:56,305
And also you can really have
as much fresh water as you want.

150
00:07:57,264 --> 00:07:58,432
Earth is mostly water.

151
00:07:58,432 --> 00:07:59,850
We should call Earth “Water.”

152
00:07:59,850 --> 00:08:01,685
It's 70 percent water by surface area.

153
00:08:01,685 --> 00:08:03,103
Now most of that’s seawater,

154
00:08:03,103 --> 00:08:06,565
but it's like we just happen to be
on the bit that's land.

155
00:08:06,565 --> 00:08:09,777
CA: And with energy,
you can turn seawater into --

156
00:08:09,777 --> 00:08:10,945
EM: Yes.

157
00:08:10,986 --> 00:08:13,322
CA: Irrigating water
or whatever water you need.

158
00:08:13,864 --> 00:08:15,074
EM: At very low cost.

159
00:08:15,115 --> 00:08:16,283
Things will be good.

160
00:08:16,325 --> 00:08:17,493
CA: Things will be good.

161
00:08:17,535 --> 00:08:20,454
And also, there's other benefits
to this non-fossil fuel world

162
00:08:20,496 --> 00:08:21,956
where the air is cleaner --

163
00:08:21,997 --> 00:08:23,207
EM: Yes, exactly.

164
00:08:23,707 --> 00:08:26,502
Because, like, when you burn fossil fuels,

165
00:08:26,544 --> 00:08:29,838
there's all these side reactions

166
00:08:29,880 --> 00:08:32,591
and toxic gases of various kinds.

167
00:08:33,509 --> 00:08:38,389
And sort of little particulates
that are bad for your lungs.

168
00:08:38,430 --> 00:08:41,058
Like, there's all sorts
of bad things that are happening

169
00:08:41,100 --> 00:08:42,268
that will go away.

170
00:08:42,268 --> 00:08:45,479
And the sky will be cleaner and quieter.

171
00:08:45,479 --> 00:08:46,981
The future's going to be good.

172
00:08:46,981 --> 00:08:50,442
CA: I want us to switch now to think
a bit about artificial intelligence.

173
00:08:50,442 --> 00:08:52,278
But the segue there,

174
00:08:52,278 --> 00:08:55,739
you mentioned how annoying it is
when people call you up

175
00:08:55,781 --> 00:08:58,659
for bad predictions in the past.

176
00:08:58,701 --> 00:09:02,913
So I'm possibly going to be annoying now,

177
00:09:02,955 --> 00:09:07,376
but I’m curious about your timelines
and how you predict

178
00:09:07,418 --> 00:09:10,796
and how come some things are so amazingly
on the money and some aren't.

179
00:09:10,796 --> 00:09:15,759
So when it comes to predicting sales
of Tesla vehicles, for example,

180
00:09:15,801 --> 00:09:17,177
you've kind of been amazing,

181
00:09:17,177 --> 00:09:22,600
I think in 2014 when Tesla
had sold that year 60,000 cars,

182
00:09:22,641 --> 00:09:26,353
you said, "2020, I think we will do
half a million a year."

183
00:09:26,395 --> 00:09:28,606
EM: Yeah, we did
almost exactly a half million.

184
00:09:28,647 --> 00:09:30,649
CA: You did almost exactly half a million.

185
00:09:30,691 --> 00:09:33,944
You were scoffed in 2014
because no one since Henry Ford,

186
00:09:33,944 --> 00:09:38,490
with the Model T, had come close
to that kind of growth rate for cars.

187
00:09:38,532 --> 00:09:41,368
You were scoffed, and you actually
hit 500,000 cars

188
00:09:41,410 --> 00:09:43,746
and then 510,000 or whatever produced.

189
00:09:44,246 --> 00:09:47,416
But five years ago,
last time you came to TED,

190
00:09:47,416 --> 00:09:50,377
I asked you about full self-driving,

191
00:09:50,419 --> 00:09:53,297
and you said, “Yeah, this very year,

192
00:09:53,297 --> 00:09:58,761
I'm confident that we will have a car
going from LA to New York

193
00:09:58,802 --> 00:10:00,846
without any intervention."

194
00:10:00,888 --> 00:10:04,099
EM: Yeah, I don't want to blow your mind,
but I'm not always right.

195
00:10:04,099 --> 00:10:05,476
CA: (Laughs)

196
00:10:05,517 --> 00:10:07,561
What's the difference between those two?

197
00:10:08,646 --> 00:10:13,317
Why has full self-driving in particular
been so hard to predict?

198
00:10:13,359 --> 00:10:15,319
EM: I mean, the thing that really got me,

199
00:10:15,361 --> 00:10:17,821
and I think it's going to get
a lot of other people,

200
00:10:17,821 --> 00:10:22,159
is that there are just so many
false dawns with self-driving,

201
00:10:22,159 --> 00:10:25,287
where you think you've got the problem,

202
00:10:25,329 --> 00:10:26,747
have a handle on the problem,

203
00:10:26,747 --> 00:10:30,959
and then it, no, turns out
you just hit a ceiling.

204
00:10:33,754 --> 00:10:37,508
Because if you were to plot the progress,

205
00:10:37,508 --> 00:10:39,259
the progress looks like a log curve.

206
00:10:39,259 --> 00:10:41,595
So it's like a series of log curves.

207
00:10:41,637 --> 00:10:44,431
So most people don't know
what a log curve is, I suppose.

208
00:10:45,474 --> 00:10:47,226
CA: Show the shape with your hands.

209
00:10:47,267 --> 00:10:50,229
EM: It goes up you know,
sort of a fairly straight way,

210
00:10:50,270 --> 00:10:52,606
and then it starts tailing off

211
00:10:52,648 --> 00:10:55,317
and you start getting diminishing returns.

212
00:10:55,693 --> 00:10:57,569
And you're like, uh oh,

213
00:10:57,611 --> 00:11:01,240
it was trending up and now
it's sort of, curving over

214
00:11:01,281 --> 00:11:05,786
and you start getting to these,
what I call local maxima,

215
00:11:05,828 --> 00:11:08,664
where you don't realize
basically how dumb you were.

216
00:11:10,040 --> 00:11:11,959
And then it happens again.

217
00:11:14,253 --> 00:11:15,546
And ultimately...

218
00:11:16,004 --> 00:11:19,299
These things, you know,
in retrospect, they seem obvious,

219
00:11:19,341 --> 00:11:23,429
but in order to solve
full self-driving properly,

220
00:11:23,429 --> 00:11:26,390
you actually have to solve real-world AI.

221
00:11:28,350 --> 00:11:32,521
Because what are the road networks
designed to work with?

222
00:11:32,563 --> 00:11:36,483
They're designed to work
with a biological neural net, our brains,

223
00:11:36,483 --> 00:11:39,778
and with vision, our eyes.

224
00:11:40,487 --> 00:11:45,534
And so in order to make it
work with computers,

225
00:11:45,576 --> 00:11:50,414
you basically need to solve
real-world AI and vision.

226
00:11:51,123 --> 00:11:56,170
Because we need cameras

227
00:11:56,211 --> 00:11:58,922
and silicon neural nets

228
00:11:58,922 --> 00:12:01,884
in order to have self-driving work

229
00:12:01,884 --> 00:12:06,096
for a system that was designed
for eyes and biological neural nets.

230
00:12:07,139 --> 00:12:09,224
You know, I guess
when you put it that way,

231
00:12:09,224 --> 00:12:10,809
it's sort of, like, quite obvious

232
00:12:10,851 --> 00:12:12,936
that the only way
to solve full self-driving

233
00:12:12,936 --> 00:12:16,899
is to solve real world AI
and sophisticated vision.

234
00:12:16,899 --> 00:12:19,359
CA: What do you feel
about the current architecture?

235
00:12:19,401 --> 00:12:21,403
Do you think you have an architecture now

236
00:12:21,403 --> 00:12:23,197
where there is a chance

237
00:12:23,197 --> 00:12:27,159
for the logarithmic curve
not to tail off any anytime soon?

238
00:12:27,743 --> 00:12:32,956
EM: Well I mean, admittedly
these may be infamous last words,

239
00:12:32,998 --> 00:12:35,834
but I actually am confident
that we will solve it this year.

240
00:12:36,126 --> 00:12:37,920
That we will exceed --

241
00:12:39,379 --> 00:12:42,341
The probability of an accident,

242
00:12:42,382 --> 00:12:44,968
at what point do you exceed
that of the average person?

243
00:12:45,344 --> 00:12:47,304
I think we will exceed that this year.

244
00:12:47,346 --> 00:12:51,058
CA: What are you seeing behind the scenes
that gives you that confidence?

245
00:12:51,099 --> 00:12:53,894
EM: We’re almost at the point
where we have a high-quality

246
00:12:53,894 --> 00:12:55,813
unified vector space.

247
00:12:55,854 --> 00:13:00,067
In the beginning, we were trying
to do this with image recognition

248
00:13:00,108 --> 00:13:03,028
on individual images.

249
00:13:03,070 --> 00:13:04,988
But if you get one image out of a video,

250
00:13:05,030 --> 00:13:09,868
it's actually quite hard to see
what's going on without ambiguity.

251
00:13:09,910 --> 00:13:13,038
But if you look at a video segment
of a few seconds of video,

252
00:13:13,080 --> 00:13:14,748
that ambiguity resolves.

253
00:13:15,791 --> 00:13:19,253
So the first thing we had to do
is tie all eight cameras together

254
00:13:19,294 --> 00:13:20,462
so they're synchronized,

255
00:13:20,504 --> 00:13:23,674
so that all the frames
are looked at simultaneously

256
00:13:23,715 --> 00:13:26,677
and labeled simultaneously by one person,

257
00:13:26,718 --> 00:13:28,595
because we still need human labeling.

258
00:13:30,180 --> 00:13:33,684
So at least they’re not labeled
at different times by different people

259
00:13:33,684 --> 00:13:34,893
in different ways.

260
00:13:35,561 --> 00:13:37,980
So it's sort of a surround picture.

261
00:13:37,980 --> 00:13:41,400
Then a very important part
is to add the time dimension.

262
00:13:41,733 --> 00:13:45,320
So that you’re looking at surround video,

263
00:13:45,320 --> 00:13:47,239
and you're labeling surround video.

264
00:13:47,865 --> 00:13:52,536
And this is actually quite difficult to do
from a software standpoint.

265
00:13:52,911 --> 00:13:57,583
We had to write our own labeling tools

266
00:13:57,624 --> 00:14:03,088
and then create auto labeling,

267
00:14:03,130 --> 00:14:07,676
create auto labeling software to amplify
the efficiency of human labelers

268
00:14:07,676 --> 00:14:09,428
because it’s quite hard to label.

269
00:14:09,469 --> 00:14:11,763
In the beginning,
it was taking several hours

270
00:14:11,763 --> 00:14:13,891
to label a 10-second video clip.

271
00:14:13,932 --> 00:14:16,101
This is not scalable.

272
00:14:16,518 --> 00:14:19,813
So basically what you have to have
is you have to have surround video,

273
00:14:19,813 --> 00:14:23,609
and that surround video has to be
primarily automatically labeled

274
00:14:23,609 --> 00:14:25,277
with humans just being editors

275
00:14:25,319 --> 00:14:29,990
and making slight corrections
to the labeling of the video

276
00:14:30,032 --> 00:14:34,661
and then feeding back those corrections
into the future auto labeler,

277
00:14:34,661 --> 00:14:36,371
so you get this flywheel eventually

278
00:14:36,371 --> 00:14:39,666
where the auto labeler
is able to take in vast amounts of video

279
00:14:39,666 --> 00:14:41,001
and with high accuracy,

280
00:14:41,043 --> 00:14:46,924
automatically label the video
for cars, lane lines, drive space.

281
00:14:46,965 --> 00:14:51,261
CA: What you’re saying is ...

282
00:14:51,303 --> 00:14:56,099
the result of this is that you're
effectively giving the car a 3D model

283
00:14:56,099 --> 00:14:58,560
of the actual objects
that are all around it.

284
00:14:58,560 --> 00:15:01,021
It knows what they are,

285
00:15:01,063 --> 00:15:03,857
and it knows how fast they are moving.

286
00:15:03,899 --> 00:15:09,237
And the remaining task is to predict

287
00:15:09,237 --> 00:15:12,616
what the quirky behaviors are
that, you know,

288
00:15:12,658 --> 00:15:16,244
that when a pedestrian is walking
down the road with a smaller pedestrian,

289
00:15:16,286 --> 00:15:19,498
that maybe that smaller pedestrian
might do something unpredictable

290
00:15:19,498 --> 00:15:21,166
or things like that.

291
00:15:21,208 --> 00:15:24,211
You have to build into it
before you can really call it safe.

292
00:15:24,211 --> 00:15:30,092
EM: You basically need to have
memory across time and space.

293
00:15:30,467 --> 00:15:33,011
So what I mean by that is ...

294
00:15:34,763 --> 00:15:37,307
Memory can’t be infinite,

295
00:15:37,349 --> 00:15:41,561
because it's using up a lot
of the computer's RAM basically.

296
00:15:42,562 --> 00:15:45,440
So you have to say how much
are you going to try to remember?

297
00:15:46,650 --> 00:15:49,277
It's very common
for things to be occluded.

298
00:15:49,319 --> 00:15:53,490
So if you talk about say,
a pedestrian walking past a truck

299
00:15:53,490 --> 00:15:57,619
where you saw the pedestrian start
on one side of the truck,

300
00:15:57,619 --> 00:15:59,705
then they're occluded by the truck.

301
00:16:01,540 --> 00:16:03,917
You would know intuitively,

302
00:16:03,917 --> 00:16:07,170
OK, that pedestrian is going to pop out
the other side, most likely.

303
00:16:07,504 --> 00:16:09,047
CA: A computer doesn't know it.

304
00:16:09,047 --> 00:16:10,340
EM: You need to slow down.

305
00:16:10,340 --> 00:16:14,219
CA: A skeptic is going to say
that every year for the last five years,

306
00:16:14,261 --> 00:16:15,929
you've kind of said, well,

307
00:16:15,971 --> 00:16:17,180
no this is the year,

308
00:16:17,222 --> 00:16:20,475
we're confident that it will be there
in a year or two or, you know,

309
00:16:20,475 --> 00:16:23,061
like it's always been about that far away.

310
00:16:23,103 --> 00:16:25,897
But we've got a new architecture now,

311
00:16:25,897 --> 00:16:28,734
you're seeing enough improvement
behind the scenes

312
00:16:28,734 --> 00:16:31,695
to make you not certain,
but pretty confident,

313
00:16:31,737 --> 00:16:33,947
that, by the end of this year,

314
00:16:33,989 --> 00:16:37,117
what in most, not in every city,
and every circumstance

315
00:16:37,159 --> 00:16:39,619
but in many cities and circumstances,

316
00:16:39,619 --> 00:16:42,497
basically the car will be able
to drive without interventions

317
00:16:42,539 --> 00:16:43,707
safer than a human.

318
00:16:43,707 --> 00:16:44,875
EM: Yes.

319
00:16:44,916 --> 00:16:47,377
I mean, the car currently
drives me around Austin

320
00:16:47,377 --> 00:16:49,421
most of the time with no interventions.

321
00:16:49,755 --> 00:16:51,506
So it's not like ...

322
00:16:52,340 --> 00:16:55,469
And we have over 100,000 people

323
00:16:55,510 --> 00:16:58,388
in our full self-driving beta program.

324
00:16:59,181 --> 00:17:02,267
So you can look at the videos
that they post online.

325
00:17:02,309 --> 00:17:03,477
CA: I do.

326
00:17:03,935 --> 00:17:07,189
And some of them are great,
and some of them are a little terrifying.

327
00:17:07,189 --> 00:17:09,357
I mean, occasionally
the car seems to veer off

328
00:17:09,357 --> 00:17:11,234
and scare the hell out of people.

329
00:17:12,152 --> 00:17:13,487
EM: It’s still a beta.

330
00:17:15,072 --> 00:17:17,657
CA: But you’re behind the scenes,
looking at the data,

331
00:17:17,699 --> 00:17:19,242
you're seeing enough improvement

332
00:17:19,242 --> 00:17:23,455
to believe that a this-year
timeline is real.

333
00:17:23,497 --> 00:17:25,207
EM: Yes, that's what it seems like.

334
00:17:25,207 --> 00:17:28,835
I mean, we could be here
talking again in a year,

335
00:17:28,877 --> 00:17:31,630
like, well, another year went by,
and it didn’t happen.

336
00:17:31,630 --> 00:17:33,340
But I think this is the year.

337
00:17:33,381 --> 00:17:36,676
CA: And so in general,
when people talk about Elon time,

338
00:17:36,718 --> 00:17:39,930
I mean it sounds like
you can't just have a general rule

339
00:17:39,930 --> 00:17:42,891
that if you predict that something
will be done in six months,

340
00:17:42,891 --> 00:17:45,811
actually what we should imagine
is it’s going to be a year

341
00:17:45,852 --> 00:17:49,106
or it’s like two-x or three-x,
it depends on the type of prediction.

342
00:17:49,106 --> 00:17:52,943
Some things, I guess,
things involving software, AI, whatever,

343
00:17:52,943 --> 00:17:56,947
are fundamentally harder
to predict than others.

344
00:17:56,988 --> 00:17:58,156
Is there an element

345
00:17:58,198 --> 00:18:01,535
that you actually deliberately make
aggressive prediction timelines

346
00:18:01,576 --> 00:18:04,996
to drive people to be ambitious?

347
00:18:04,996 --> 00:18:06,706
Without that, nothing gets done?

348
00:18:06,748 --> 00:18:09,751
EM: Well, I generally believe,
in terms of internal timelines,

349
00:18:09,793 --> 00:18:14,673
that we want to set the most aggressive
timeline that we can.

350
00:18:14,673 --> 00:18:17,843
Because there’s sort of like
a law of gaseous expansion where,

351
00:18:17,843 --> 00:18:20,929
for schedules, where
whatever time you set,

352
00:18:20,971 --> 00:18:22,722
it's not going to be less than that.

353
00:18:22,722 --> 00:18:24,975
It's very rare
that it'll be less than that.

354
00:18:26,184 --> 00:18:28,270
But as far as our predictions
are concerned,

355
00:18:28,311 --> 00:18:29,896
what tends to happen in the media

356
00:18:29,938 --> 00:18:31,940
is that they will report
all the wrong ones

357
00:18:31,982 --> 00:18:33,483
and ignore all the right ones.

358
00:18:33,525 --> 00:18:38,321
Or, you know, when writing
an article about me --

359
00:18:38,321 --> 00:18:40,532
I've had a long career
in multiple industries.

360
00:18:40,574 --> 00:18:43,410
If you list my sins, I sound
like the worst person on Earth.

361
00:18:43,410 --> 00:18:46,788
But if you put those
against the things I've done right,

362
00:18:46,788 --> 00:18:48,582
it makes much more sense, you know?

363
00:18:48,623 --> 00:18:51,376
So essentially like,
the longer you do anything,

364
00:18:51,418 --> 00:18:55,088
the more mistakes
that you will make cumulatively.

365
00:18:55,130 --> 00:18:56,923
Which, if you sum up those mistakes,

366
00:18:56,965 --> 00:19:00,594
will sound like I'm the worst
predictor ever.

367
00:19:00,635 --> 00:19:04,514
But for example, for Tesla vehicle growth,

368
00:19:04,556 --> 00:19:08,935
I said I think we’d do 50 percent,
and we’ve done 80 percent.

369
00:19:08,977 --> 00:19:10,103
CA: Yes.

370
00:19:10,937 --> 00:19:12,689
EM: But they don't mention that one.

371
00:19:12,689 --> 00:19:16,443
So, I mean, I'm not sure what my exact
track record is on predictions.

372
00:19:16,484 --> 00:19:19,946
They're more optimistic than pessimistic,
but they're not all optimistic.

373
00:19:19,946 --> 00:19:24,784
Some of them are exceeded
probably more or later,

374
00:19:24,826 --> 00:19:28,747
but they do come true.

375
00:19:28,747 --> 00:19:31,625
It's very rare that they do not come true.

376
00:19:31,666 --> 00:19:34,878
It's sort of like, you know,

377
00:19:34,878 --> 00:19:38,548
if there's some radical
technology prediction,

378
00:19:38,590 --> 00:19:40,759
the point is not
that it was a few years late,

379
00:19:40,800 --> 00:19:42,344
but that it happened at all.

380
00:19:43,136 --> 00:19:45,472
That's the more important part.

381
00:19:45,889 --> 00:19:49,851
CA: So it feels like
at some point in the last year,

382
00:19:49,851 --> 00:19:54,522
seeing the progress on understanding,

383
00:19:54,522 --> 00:19:57,943
the Tesla AI understanding
the world around it,

384
00:19:57,984 --> 00:20:00,528
led to a kind of, an aha moment at Tesla.

385
00:20:00,570 --> 00:20:03,657
Because you really surprised people
recently when you said

386
00:20:03,698 --> 00:20:06,201
probably the most important
product development

387
00:20:06,243 --> 00:20:10,121
going on at Tesla this year
is this robot, Optimus.

388
00:20:10,163 --> 00:20:11,331
EM: Yes.

389
00:20:11,331 --> 00:20:14,542
CA: Many companies out there
have tried to put out these robots,

390
00:20:14,584 --> 00:20:16,503
they've been working on them for years.

391
00:20:16,503 --> 00:20:18,797
And so far no one has really cracked it.

392
00:20:18,797 --> 00:20:22,592
There's no mass adoption
robot in people's homes.

393
00:20:22,592 --> 00:20:25,762
There are some in manufacturing,
but I would say,

394
00:20:25,762 --> 00:20:28,473
no one's kind of, really cracked it.

395
00:20:29,724 --> 00:20:31,518
Is it something that happened

396
00:20:31,559 --> 00:20:35,146
in the development of full self-driving
that gave you the confidence to say,

397
00:20:35,146 --> 00:20:37,607
"You know what, we could do
something special here."

398
00:20:37,607 --> 00:20:38,858
EM: Yeah, exactly.

399
00:20:38,858 --> 00:20:41,528
So, you know, it took me a while
to sort of realize

400
00:20:41,528 --> 00:20:44,447
that in order to solve self-driving,

401
00:20:44,489 --> 00:20:46,658
you really needed to solve real-world AI.

402
00:20:47,617 --> 00:20:50,662
And at the point of which you solve
real-world AI for a car,

403
00:20:50,704 --> 00:20:53,206
which is really a robot on four wheels,

404
00:20:53,248 --> 00:20:57,669
you can then generalize that
to a robot on legs as well.

405
00:20:58,128 --> 00:20:59,546
The two hard parts I think --

406
00:20:59,587 --> 00:21:02,173
like obviously companies
like Boston Dynamics

407
00:21:02,215 --> 00:21:05,093
have shown that it's possible
to make quite compelling,

408
00:21:05,135 --> 00:21:06,761
sometimes alarming robots.

409
00:21:06,761 --> 00:21:07,971
CA: Right.

410
00:21:08,013 --> 00:21:11,308
EM: You know, so from a sensors
and actuators standpoint,

411
00:21:11,349 --> 00:21:14,602
it's certainly been demonstrated by many

412
00:21:14,644 --> 00:21:16,730
that it's possible to make
a humanoid robot.

413
00:21:16,730 --> 00:21:22,277
The things that are currently missing
are enough intelligence

414
00:21:22,277 --> 00:21:25,363
for the robot to navigate the real world
and do useful things

415
00:21:25,405 --> 00:21:27,657
without being explicitly instructed.

416
00:21:27,657 --> 00:21:31,786
So the missing things are basically
real-world intelligence

417
00:21:31,786 --> 00:21:34,289
and scaling up manufacturing.

418
00:21:34,664 --> 00:21:37,083
Those are two things
that Tesla is very good at.

419
00:21:37,125 --> 00:21:43,131
And so then we basically just need
to design the specialized actuators

420
00:21:43,173 --> 00:21:45,508
and sensors that are needed
for humanoid robot.

421
00:21:46,301 --> 00:21:49,346
People have no idea,
this is going to be bigger than the car.

422
00:21:50,764 --> 00:21:52,682
CA: So let's dig into exactly that.

423
00:21:52,724 --> 00:21:56,311
I mean, in one way, it's actually
an easier problem than full self-driving

424
00:21:56,311 --> 00:22:00,106
because instead of an object
going along at 60 miles an hour,

425
00:22:00,148 --> 00:22:02,692
which if it gets it wrong,
someone will die.

426
00:22:02,692 --> 00:22:05,278
This is an object that's engineered
to only go at what,

427
00:22:05,320 --> 00:22:07,322
three or four or five miles an hour.

428
00:22:07,322 --> 00:22:10,325
And so a mistake,
there aren't lives at stake.

429
00:22:10,367 --> 00:22:12,202
There might be embarrassment at stake.

430
00:22:12,202 --> 00:22:17,332
EM: So long as the AI doesn't take it over
and murder us in our sleep or something.

431
00:22:17,374 --> 00:22:18,541
CA: Right.

432
00:22:18,583 --> 00:22:19,751
(Laughter)

433
00:22:20,794 --> 00:22:22,337
So talk about --

434
00:22:22,379 --> 00:22:25,090
I think the first applications
you've mentioned

435
00:22:25,131 --> 00:22:27,050
are probably going to be manufacturing,

436
00:22:27,050 --> 00:22:30,512
but eventually the vision is to have
these available for people at home.

437
00:22:30,553 --> 00:22:36,851
If you had a robot that really understood
the 3D architecture of your house

438
00:22:36,893 --> 00:22:41,898
and knew where every object
in that house was

439
00:22:41,940 --> 00:22:43,274
or was supposed to be,

440
00:22:43,274 --> 00:22:46,111
and could recognize all those objects,

441
00:22:46,152 --> 00:22:48,405
I mean, that’s kind of amazing, isn’t it?

442
00:22:48,446 --> 00:22:51,741
Like the kind of thing
that you could ask a robot to do

443
00:22:51,741 --> 00:22:52,909
would be what?

444
00:22:52,951 --> 00:22:54,160
Like, tidy up?

445
00:22:54,160 --> 00:22:55,620
EM: Yeah, absolutely.

446
00:22:57,122 --> 00:22:59,374
Make dinner, I guess, mow the lawn.

447
00:22:59,416 --> 00:23:03,795
CA: Take a cup of tea to grandma
and show her family pictures.

448
00:23:04,462 --> 00:23:08,758
EM: Exactly. Take care
of my grandmother and make sure --

449
00:23:08,758 --> 00:23:11,386
CA: It could obviously recognize
everyone in the home.

450
00:23:12,053 --> 00:23:13,930
It could play catch with your kids.

451
00:23:13,972 --> 00:23:16,266
EM: Yes. I mean, obviously,
we need to be careful

452
00:23:16,307 --> 00:23:18,852
this doesn't become a dystopian situation.

453
00:23:20,520 --> 00:23:23,064
I think one of the things
that's going to be important

454
00:23:23,064 --> 00:23:26,568
is to have a localized
ROM chip on the robot

455
00:23:26,609 --> 00:23:29,654
that cannot be updated over the air.

456
00:23:29,696 --> 00:23:32,490
Where if you, for example, were to say,
“Stop, stop, stop,”

457
00:23:32,490 --> 00:23:33,950
if anyone said that,

458
00:23:33,992 --> 00:23:36,411
then the robot would stop,
you know, type of thing.

459
00:23:36,411 --> 00:23:38,371
And that's not updatable remotely.

460
00:23:38,997 --> 00:23:42,250
I think it's going to be important
to have safety features like that.

461
00:23:42,292 --> 00:23:43,793
CA: Yeah, that sounds wise.

462
00:23:43,793 --> 00:23:46,754
EM: And I do think there should be
a regulatory agency for AI.

463
00:23:46,754 --> 00:23:48,214
I've said that for many years.

464
00:23:48,214 --> 00:23:49,632
I don't love being regulated,

465
00:23:49,632 --> 00:23:52,343
but I think this is an important
thing for public safety.

466
00:23:52,343 --> 00:23:53,720
CA: Let's come back to that.

467
00:23:53,720 --> 00:23:58,183
But I don't think many people
have really sort of taken seriously

468
00:23:58,183 --> 00:24:00,935
the notion of, you know, a robot at home.

469
00:24:00,935 --> 00:24:03,229
I mean, at the start
of the computing revolution,

470
00:24:03,271 --> 00:24:06,149
Bill Gates said there's going to be
a computer in every home.

471
00:24:06,149 --> 00:24:09,444
And people at the time said, yeah,
whatever, who would even want that.

472
00:24:09,903 --> 00:24:13,573
Do you think there will be basically
like in, say, 2050 or whatever,

473
00:24:13,615 --> 00:24:17,785
like a robot in most homes,
is what there will be,

474
00:24:17,827 --> 00:24:20,246
and people will love them
and count on them?

475
00:24:20,663 --> 00:24:22,499
You’ll have your own butler basically.

476
00:24:22,749 --> 00:24:26,669
EM: Yeah, you'll have your sort of
buddy robot probably, yeah.

477
00:24:27,003 --> 00:24:28,588
CA: I mean, how much of a buddy?

478
00:24:28,630 --> 00:24:31,257
How many applications have you thought,

479
00:24:31,257 --> 00:24:34,093
you know, can you have
a romantic partner, a sex partner?

480
00:24:34,135 --> 00:24:36,262
EM: It's probably inevitable.

481
00:24:36,304 --> 00:24:39,098
I mean, I did promise the internet
that I’d make catgirls.

482
00:24:39,098 --> 00:24:41,142
We could make a robot catgirl.

483
00:24:42,644 --> 00:24:44,812
CA: Be careful what
you promise the internet.

484
00:24:44,812 --> 00:24:47,065
(Laughter)

485
00:24:47,065 --> 00:24:52,028
EM: So, yeah, I guess it'll be
whatever people want really, you know.

486
00:24:52,487 --> 00:24:56,157
CA: What sort of timeline
should we be thinking about

487
00:24:56,157 --> 00:24:59,994
of the first models
that are actually made and sold?

488
00:25:01,621 --> 00:25:05,333
EM: Well, you know, the first units
that we intend to make

489
00:25:05,375 --> 00:25:10,004
are for jobs that are dangerous,
boring, repetitive,

490
00:25:10,004 --> 00:25:11,923
and things that people don't want to do.

491
00:25:11,965 --> 00:25:15,009
And, you know, I think we’ll have like
an interesting prototype

492
00:25:15,009 --> 00:25:16,219
sometime this year.

493
00:25:16,219 --> 00:25:18,846
We might have something useful next year,

494
00:25:18,888 --> 00:25:21,724
but I think quite likely
within at least two years.

495
00:25:22,308 --> 00:25:24,477
And then we'll see
rapid growth year over year

496
00:25:24,519 --> 00:25:27,063
of the usefulness
of the humanoid robots

497
00:25:27,105 --> 00:25:29,816
and decrease in cost
and scaling up production.

498
00:25:29,857 --> 00:25:31,818
CA: Initially just selling to businesses,

499
00:25:31,859 --> 00:25:34,529
or when do you picture
you'll start selling them

500
00:25:34,571 --> 00:25:38,866
where you can buy your parents one
for Christmas or something?

501
00:25:39,450 --> 00:25:41,160
EM: I'd say in less than ten years.

502
00:25:41,160 --> 00:25:43,997
CA: Help me on the economics of this.

503
00:25:43,997 --> 00:25:47,208
So what do you picture the cost
of one of these being?

504
00:25:47,250 --> 00:25:50,545
EM: Well, I think the cost is actually
not going to be crazy high.

505
00:25:51,921 --> 00:25:53,172
Like less than a car.

506
00:25:53,172 --> 00:25:56,426
Initially, things will be expensive
because it'll be a new technology

507
00:25:56,467 --> 00:25:57,677
at low production volume.

508
00:25:57,677 --> 00:26:01,347
The complexity and cost of a car
is greater than that of a humanoid robot.

509
00:26:01,681 --> 00:26:05,893
So I would expect that it's going
to be less than a car,

510
00:26:05,935 --> 00:26:07,770
or at least equivalent to a cheap car.

511
00:26:07,770 --> 00:26:10,231
CA: So even if it starts at 50k,
within a few years,

512
00:26:10,273 --> 00:26:12,609
it’s down to 20k or lower or whatever.

513
00:26:13,568 --> 00:26:15,903
And maybe for home
they'll get much cheaper still.

514
00:26:15,903 --> 00:26:17,780
But think about the economics of this.

515
00:26:17,822 --> 00:26:22,076
If you can replace a $30,000,

516
00:26:22,076 --> 00:26:24,871
$40,000-a-year worker,

517
00:26:24,912 --> 00:26:26,581
which you have to pay every year,

518
00:26:26,623 --> 00:26:29,667
with a one-time payment of $25,000

519
00:26:29,667 --> 00:26:32,587
for a robot that can work longer hours,

520
00:26:32,629 --> 00:26:36,633
a pretty rapid replacement
of certain types of jobs.

521
00:26:36,674 --> 00:26:38,885
How worried should
the world be about that?

522
00:26:39,344 --> 00:26:42,847
EM: I wouldn't worry about the sort of,
putting people out of a job thing.

523
00:26:42,889 --> 00:26:46,225
I think we're actually going to have,
and already do have,

524
00:26:46,267 --> 00:26:47,644
a massive shortage of labor.

525
00:26:47,644 --> 00:26:50,563
So I think we will have ...

526
00:26:54,067 --> 00:26:55,276
Not people out of work,

527
00:26:55,276 --> 00:26:58,112
but actually still a shortage
labor even in the future.

528
00:26:58,863 --> 00:27:03,493
But this really will be
a world of abundance.

529
00:27:03,534 --> 00:27:08,498
Any goods and services will be available
to anyone who wants them.

530
00:27:08,790 --> 00:27:12,001
It'll be so cheap to have goods
and services, it will be ridiculous.

531
00:27:12,043 --> 00:27:16,089
CA: I'm presuming it should be possible
to imagine a bunch of goods and services

532
00:27:16,089 --> 00:27:20,510
that can't profitably be made now
but could be made in that world,

533
00:27:20,551 --> 00:27:23,179
courtesy of legions of robots.

534
00:27:23,179 --> 00:27:24,639
EM: Yeah.

535
00:27:25,014 --> 00:27:26,808
It will be a world of abundance.

536
00:27:26,808 --> 00:27:29,310
The only scarcity
that will exist in the future

537
00:27:29,352 --> 00:27:32,522
is that which we decide to create
ourselves as humans.

538
00:27:32,563 --> 00:27:33,731
CA: OK.

539
00:27:33,731 --> 00:27:38,069
So AI is allowing us to imagine
a differently powered economy

540
00:27:38,111 --> 00:27:40,238
that will create this abundance.

541
00:27:40,279 --> 00:27:42,365
What are you most worried
about going wrong?

542
00:27:42,407 --> 00:27:48,413
EM: Well, like I said,
AI and robotics will bring out

543
00:27:48,454 --> 00:27:50,998
what might be termed the age of abundance.

544
00:27:51,332 --> 00:27:53,292
Other people have used this word,

545
00:27:54,210 --> 00:27:55,878
and that this is my prediction:

546
00:27:55,920 --> 00:27:59,132
it will be an age of abundance 
for everyone.

547
00:27:59,841 --> 00:28:02,385
But I guess there’s ...

548
00:28:03,886 --> 00:28:08,433
The dangers would be
the artificial general intelligence

549
00:28:08,474 --> 00:28:13,730
or digital superintelligence decouples
from a collective human will

550
00:28:13,771 --> 00:28:17,275
and goes in the direction
that for some reason we don't like.

551
00:28:17,316 --> 00:28:19,444
Whatever direction it might go.

552
00:28:20,570 --> 00:28:23,990
You know, that’s sort of
the idea behind Neuralink,

553
00:28:24,031 --> 00:28:27,076
is to try to more tightly couple
collective human world

554
00:28:27,076 --> 00:28:31,539
to digital superintelligence.

555
00:28:33,458 --> 00:28:39,213
And also along the way solve a lot
of brain injuries and spinal injuries

556
00:28:39,213 --> 00:28:40,381
and that kind of thing.

557
00:28:40,423 --> 00:28:42,759
So even if it doesn't succeed
in the greater goal,

558
00:28:42,759 --> 00:28:48,347
I think it will succeed in the goal
of alleviating brain and spine damage.

559
00:28:48,347 --> 00:28:51,392
CA: So the spirit there is
that if we're going to make these AIs

560
00:28:51,434 --> 00:28:54,687
that are so vastly intelligent,
we ought to be wired directly to them

561
00:28:54,687 --> 00:28:59,108
so that we ourselves can have
those superpowers more directly.

562
00:28:59,609 --> 00:29:04,030
But that doesn't seem to avoid
the risk that those superpowers might ...

563
00:29:05,740 --> 00:29:08,326
turn ugly in unintended ways.

564
00:29:08,326 --> 00:29:09,952
EM: I think it's a risk, I agree.

565
00:29:09,994 --> 00:29:16,250
I'm not saying that I have
some certain answer to that risk.

566
00:29:16,292 --> 00:29:18,586
I’m just saying like

567
00:29:18,628 --> 00:29:22,173
maybe one of the things
that would be good

568
00:29:22,215 --> 00:29:27,887
for ensuring that the future
is one that we want

569
00:29:27,887 --> 00:29:31,140
is to more tightly couple

570
00:29:31,140 --> 00:29:34,894
the collective human world
to digital intelligence.

571
00:29:36,437 --> 00:29:40,566
The issue that we face here
is that we are already a cyborg,

572
00:29:40,566 --> 00:29:41,818
if you think about it.

573
00:29:41,859 --> 00:29:45,822
The computers are
an extension of ourselves.

574
00:29:46,697 --> 00:29:49,700
And when we die, we have,
like, a digital ghost.

575
00:29:49,742 --> 00:29:53,287
You know, all of our text messages
and social media, emails.

576
00:29:53,329 --> 00:29:55,331
And it's quite eerie actually,

577
00:29:55,373 --> 00:29:58,668
when someone dies but everything
online is still there.

578
00:29:59,001 --> 00:30:00,920
But you say like, what's the limitation?

579
00:30:00,962 --> 00:30:06,133
What is it that inhibits
a human-machine symbiosis?

580
00:30:06,175 --> 00:30:07,385
It's the data rate.

581
00:30:07,385 --> 00:30:09,554
When you communicate,
especially with a phone,

582
00:30:09,554 --> 00:30:12,473
you're moving your thumbs very slowly.

583
00:30:12,515 --> 00:30:15,393
So you're like moving
your two little meat sticks

584
00:30:15,393 --> 00:30:21,315
at a rate that’s maybe 10 bits per second,
optimistically, 100 bits per second.

585
00:30:21,315 --> 00:30:26,445
And computers are communicating
at the gigabyte level and beyond.

586
00:30:26,487 --> 00:30:29,657
CA: Have you seen evidence
that the technology is actually working,

587
00:30:29,657 --> 00:30:33,244
that you've got a richer, sort of,
higher bandwidth connection, if you like,

588
00:30:33,286 --> 00:30:36,122
between like external
electronics and a brain

589
00:30:36,122 --> 00:30:37,790
than has been possible before?

590
00:30:38,165 --> 00:30:39,375
EM: Yeah.

591
00:30:41,002 --> 00:30:46,424
I mean, the fundamental principles
of reading neurons,

592
00:30:46,465 --> 00:30:50,386
sort of doing read-write on neurons
with tiny electrodes,

593
00:30:50,386 --> 00:30:52,555
have been demonstrated for decades.

594
00:30:53,306 --> 00:30:57,560
So it's not like the concept is new.

595
00:30:57,602 --> 00:31:02,356
The problem is that there is
no product that works well

596
00:31:02,398 --> 00:31:04,901
that you can go and buy.

597
00:31:04,942 --> 00:31:07,528
So it's all sort of, in research labs.

598
00:31:08,738 --> 00:31:14,410
And it's like some cords
sticking out of your head.

599
00:31:14,410 --> 00:31:17,663
And it's quite gruesome,
and it's really ...

600
00:31:18,539 --> 00:31:22,627
There's no good product
that actually does a good job

601
00:31:22,627 --> 00:31:24,503
and is high-bandwidth and safe

602
00:31:24,545 --> 00:31:27,798
and something actually that you could buy
and would want to buy.

603
00:31:29,550 --> 00:31:33,471
But the way to think
of the Neuralink device

604
00:31:33,512 --> 00:31:37,058
is kind of like a Fitbit
or an Apple Watch.

605
00:31:37,934 --> 00:31:42,647
That's where we take out
sort of a small section of skull

606
00:31:42,647 --> 00:31:44,231
about the size of a quarter,

607
00:31:44,273 --> 00:31:46,525
replace that with what,

608
00:31:46,525 --> 00:31:52,198
in many ways really is very much like
a Fitbit, Apple Watch

609
00:31:52,239 --> 00:31:54,617
or some kind of smart watch thing.

610
00:31:56,035 --> 00:32:00,206
But with tiny, tiny wires,

611
00:32:00,247 --> 00:32:02,124
very, very tiny wires.

612
00:32:02,124 --> 00:32:04,168
Wires so tiny, it’s hard to even see them.

613
00:32:05,044 --> 00:32:07,254
And it's very important
to have very tiny wires

614
00:32:07,296 --> 00:32:10,132
so that when they’re implanted,
they don’t damage the brain.

615
00:32:10,132 --> 00:32:13,010
CA: How far are you from putting
these into humans?

616
00:32:14,136 --> 00:32:18,599
EM: Well, we have put in
our FDA application

617
00:32:18,641 --> 00:32:22,937
to aspirationally do the first
human implant this year.

618
00:32:23,312 --> 00:32:26,857
CA: The first uses will be
for neurological injuries

619
00:32:26,899 --> 00:32:28,192
of different kinds.

620
00:32:28,192 --> 00:32:29,735
But rolling the clock forward

621
00:32:29,777 --> 00:32:33,823
and imagining when people
are actually using these

622
00:32:33,864 --> 00:32:36,492
for their own enhancement, let's say,

623
00:32:36,534 --> 00:32:38,411
and for the enhancement of the world,

624
00:32:38,452 --> 00:32:39,912
how clear are you in your mind

625
00:32:39,912 --> 00:32:45,251
as to what it will feel like
to have one of these inside your head?

626
00:32:45,251 --> 00:32:49,171
EM: Well, I do want to emphasize
we're at an early stage.

627
00:32:49,171 --> 00:32:55,219
And so it really will be
many years before we have

628
00:32:55,261 --> 00:33:01,934
anything approximating
a high-bandwidth neural interface

629
00:33:01,934 --> 00:33:05,604
that allows for AI-human symbiosis.

630
00:33:07,481 --> 00:33:11,736
For many years, we will just be solving
brain injuries and spinal injuries.

631
00:33:11,777 --> 00:33:13,946
For probably a decade.

632
00:33:14,905 --> 00:33:18,409
This is not something
that will suddenly one day

633
00:33:18,451 --> 00:33:23,164
it will have this incredible
sort of whole brain interface.

634
00:33:25,041 --> 00:33:26,542
It's going to be, like I said,

635
00:33:26,542 --> 00:33:30,463
at least a decade of really
just solving brain injuries

636
00:33:30,504 --> 00:33:32,631
and spinal injuries.

637
00:33:32,631 --> 00:33:36,385
And really, I think you can solve
a very wide range of brain injuries,

638
00:33:36,385 --> 00:33:43,184
including severe depression,
morbid obesity, sleep,

639
00:33:43,225 --> 00:33:44,477
potentially schizophrenia,

640
00:33:44,518 --> 00:33:48,230
like, a lot of things that cause
great stress to people.

641
00:33:48,773 --> 00:33:51,776
Restoring memory in older people.

642
00:33:51,817 --> 00:33:55,613
CA: If you can pull that off,
that's the app I will sign up for.

643
00:33:56,363 --> 00:33:57,656
EM: Absolutely.

644
00:33:57,698 --> 00:33:59,867
CA: Please hurry. (Laughs)

645
00:33:59,867 --> 00:34:04,747
EM: I mean, the emails that we get
at Neuralink are heartbreaking.

646
00:34:05,122 --> 00:34:09,210
I mean, they'll send us
just tragic, you know,

647
00:34:09,251 --> 00:34:11,587
where someone was sort of,
in the prime of life

648
00:34:11,629 --> 00:34:15,424
and they had an accident on a motorcycle

649
00:34:15,466 --> 00:34:21,263
and someone who's 25, you know,
can't even feed themselves.

650
00:34:21,263 --> 00:34:23,641
And this is something we could fix.

651
00:34:24,391 --> 00:34:27,978
CA: But you have said that AI is one
of the things you're most worried about

652
00:34:28,020 --> 00:34:30,898
and that Neuralink may be one of the ways

653
00:34:30,940 --> 00:34:34,777
where we can keep abreast of it.

654
00:34:35,528 --> 00:34:39,073
EM: Yeah, there's the short-term thing,

655
00:34:39,115 --> 00:34:43,035
which I think is helpful on an individual
human level with injuries.

656
00:34:43,077 --> 00:34:45,621
And then the long-term thing is an attempt

657
00:34:45,663 --> 00:34:51,669
to address the civilizational risk of AI

658
00:34:51,710 --> 00:34:55,631
by bringing digital intelligence

659
00:34:55,673 --> 00:34:58,342
and biological intelligence
closer together.

660
00:34:58,384 --> 00:35:00,761
I mean, if you think of how
the brain works today,

661
00:35:00,803 --> 00:35:02,805
there are really two layers to the brain.

662
00:35:02,847 --> 00:35:04,807
There's the limbic system and the cortex.

663
00:35:04,849 --> 00:35:06,976
You've got the kind of,
animal brain where --

664
00:35:06,976 --> 00:35:08,853
it’s kind of like the fun part, really.

665
00:35:08,853 --> 00:35:11,355
CA: It's where most of Twitter
operates, by the way.

666
00:35:11,355 --> 00:35:13,065
EM: I think Tim Urban said,

667
00:35:13,107 --> 00:35:17,570
we’re like somebody, you know,
stuck a computer on a monkey.

668
00:35:18,320 --> 00:35:21,907
You know, so we're like,
if you gave a monkey a computer,

669
00:35:21,949 --> 00:35:23,159
that's our cortex.

670
00:35:23,159 --> 00:35:25,327
But we still have a lot
of monkey instincts.

671
00:35:25,369 --> 00:35:29,123
Which we then try to rationalize
as, no, it's not a monkey instinct.

672
00:35:29,165 --> 00:35:31,083
It’s something more important than that.

673
00:35:31,083 --> 00:35:33,210
But it's often just really
a monkey instinct.

674
00:35:33,252 --> 00:35:36,630
We're just monkeys with a computer
stuck in our brain.

675
00:35:38,883 --> 00:35:41,802
But even though the cortex
is sort of the smart,

676
00:35:41,844 --> 00:35:43,637
or the intelligent part of the brain,

677
00:35:43,679 --> 00:35:46,098
the thinking part of the brain,

678
00:35:46,098 --> 00:35:49,810
I've not yet met anyone
who wants to delete their limbic system

679
00:35:49,852 --> 00:35:51,020
or their cortex.

680
00:35:51,020 --> 00:35:52,563
They're quite happy having both.

681
00:35:52,605 --> 00:35:54,607
Everyone wants both parts of their brain.

682
00:35:56,025 --> 00:35:58,694
And people really want their
phones and their computers,

683
00:35:58,736 --> 00:36:02,239
which are really the tertiary,
the third part of your intelligence.

684
00:36:02,281 --> 00:36:03,908
It's just that it's ...

685
00:36:03,908 --> 00:36:06,202
Like the bandwidth,

686
00:36:06,202 --> 00:36:10,831
the rate of communication
with that tertiary layer is slow.

687
00:36:11,665 --> 00:36:15,461
And it's just a very tiny straw
to this tertiary layer.

688
00:36:15,502 --> 00:36:18,255
And we want to make that tiny
straw a big highway.

689
00:36:19,298 --> 00:36:22,843
And I’m definitely not saying
that this is going to solve everything.

690
00:36:22,885 --> 00:36:26,388
Or this is you know,
it’s the only thing --

691
00:36:26,430 --> 00:36:30,184
it’s something that might be helpful.

692
00:36:30,517 --> 00:36:32,228
And worst-case scenario,

693
00:36:32,228 --> 00:36:35,731
I think we solve
some important brain injury,

694
00:36:35,773 --> 00:36:38,359
spinal injury issues,
and that's still a great outcome.

695
00:36:38,359 --> 00:36:39,526
CA: Best-case scenario,

696
00:36:39,568 --> 00:36:41,987
we may discover new
human possibility, telepathy,

697
00:36:42,029 --> 00:36:46,700
you've spoken of, in a way,
a connection with a loved one, you know,

698
00:36:46,742 --> 00:36:51,747
full memory and much faster
thought processing maybe.

699
00:36:51,747 --> 00:36:53,082
All these things.

700
00:36:53,540 --> 00:36:54,750
It's very cool.

701
00:36:55,542 --> 00:37:00,965
If AI were to take down Earth,
we need a plan B.

702
00:37:01,006 --> 00:37:04,551
Let's shift our attention to space.

703
00:37:04,593 --> 00:37:06,679
We spoke last time at TED
about reusability,

704
00:37:06,720 --> 00:37:09,932
and you had just demonstrated that
spectacularly for the first time.

705
00:37:09,974 --> 00:37:15,896
Since then, you've gone on to build
this monster rocket, Starship,

706
00:37:15,938 --> 00:37:19,984
which kind of changes the rules
of the game in spectacular ways.

707
00:37:20,025 --> 00:37:21,568
Tell us about Starship.

708
00:37:22,486 --> 00:37:24,363
EM: Starship is extremely fundamental.

709
00:37:24,405 --> 00:37:30,244
So the holy grail of rocketry
or space transport

710
00:37:30,286 --> 00:37:32,079
is full and rapid reusability.

711
00:37:32,121 --> 00:37:33,539
This has never been achieved.

712
00:37:33,580 --> 00:37:36,917
The closest that anything has come
is our Falcon 9 rocket,

713
00:37:36,959 --> 00:37:42,172
where we are able to recover
the first stage, the boost stage,

714
00:37:42,214 --> 00:37:46,844
which is probably about 60 percent
of the cost of the vehicle

715
00:37:46,885 --> 00:37:49,805
of the whole launch, maybe 70 percent.

716
00:37:50,347 --> 00:37:53,517
And we've now done that
over a hundred times.

717
00:37:53,517 --> 00:37:59,648
So with Starship, we will be
recovering the entire thing.

718
00:37:59,690 --> 00:38:01,233
Or at least that's the goal.

719
00:38:01,275 --> 00:38:02,484
CA: Right.

720
00:38:02,526 --> 00:38:05,654
EM: And moreover,
recovering it in such a way

721
00:38:05,696 --> 00:38:08,324
that it can be immediately re-flown.

722
00:38:08,365 --> 00:38:11,827
Whereas with Falcon 9, we still need
to do some amount of refurbishment

723
00:38:11,869 --> 00:38:14,788
to the booster and
to the fairing nose cone.

724
00:38:16,790 --> 00:38:21,670
But with Starship, the design goal
is immediate re-flight.

725
00:38:22,212 --> 00:38:25,883
So you just refill
propellants and go again.

726
00:38:28,302 --> 00:38:30,637
And this is gigantic.

727
00:38:30,679 --> 00:38:33,557
Just as it would be
in any other mode of transport.

728
00:38:33,557 --> 00:38:35,309
CA: And the main design

729
00:38:35,351 --> 00:38:41,357
is to basically take
100 plus people at a time,

730
00:38:41,357 --> 00:38:45,194
plus a bunch of things
that they need, to Mars.

731
00:38:45,611 --> 00:38:47,571
So, first of all, talk about that piece.

732
00:38:47,613 --> 00:38:51,075
What is your latest timeline?

733
00:38:51,116 --> 00:38:54,495
One, for the first time,
a Starship goes to Mars,

734
00:38:54,536 --> 00:38:56,747
presumably without people,
but just equipment.

735
00:38:57,122 --> 00:38:58,999
Two, with people.

736
00:38:59,041 --> 00:39:01,293
Three, there’s sort of,

737
00:39:01,335 --> 00:39:04,046
OK, 100 people at a time, let's go.

738
00:39:04,546 --> 00:39:05,672
EM: Sure.

739
00:39:05,714 --> 00:39:09,510
And just to put the cost
thing into perspective,

740
00:39:09,510 --> 00:39:14,264
the expected cost of Starship,

741
00:39:14,306 --> 00:39:16,308
putting 100 tons into orbit,

742
00:39:16,350 --> 00:39:21,230
is significantly less
than what it would have cost

743
00:39:21,271 --> 00:39:25,776
or what it did cost to put our tiny
Falcon 1 rocket into orbit.

744
00:39:27,611 --> 00:39:32,282
Just as the cost of flying
a 747 around the world

745
00:39:32,282 --> 00:39:34,701
is less than the cost of a small airplane.

746
00:39:35,244 --> 00:39:37,830
You know, a small airplane
that was thrown away.

747
00:39:37,871 --> 00:39:43,919
So it's really pretty mind-boggling
that the giant thing costs less,

748
00:39:43,961 --> 00:39:45,421
way less than the small thing.

749
00:39:45,421 --> 00:39:50,008
So it doesn't use exotic propellants

750
00:39:50,050 --> 00:39:52,511
or things that are difficult
to obtain on Mars.

751
00:39:52,928 --> 00:39:56,890
It uses methane as fuel,

752
00:39:56,890 --> 00:40:02,688
and it's primarily oxygen,
roughly 77-78 percent oxygen by weight.

753
00:40:03,313 --> 00:40:06,900
And Mars has a CO2 atmosphere
and has water ice,

754
00:40:06,942 --> 00:40:10,320
which is CO2 plus H2O,
so you can make CH4, methane,

755
00:40:10,362 --> 00:40:12,156
and O2, oxygen, on Mars.

756
00:40:12,197 --> 00:40:16,160
CA: Presumably, one of the first tasks
on Mars will be to create a fuel plant

757
00:40:16,201 --> 00:40:20,456
that can create the fuel
for the return trips of many Starships.

758
00:40:20,497 --> 00:40:21,665
EM: Yes.

759
00:40:21,665 --> 00:40:24,585
And actually, it's mostly
going to be oxygen plants,

760
00:40:24,626 --> 00:40:30,591
because it's 78 percent oxygen,
22 percent fuel.

761
00:40:31,300 --> 00:40:35,012
But the fuel is a simple fuel
that is easy to create on Mars.

762
00:40:35,512 --> 00:40:38,098
And in many other parts
of the solar system.

763
00:40:38,098 --> 00:40:39,391
So basically ...

764
00:40:39,933 --> 00:40:43,729
And it's all propulsive landing,
no parachutes,

765
00:40:43,729 --> 00:40:45,189
nothing thrown away.

766
00:40:46,857 --> 00:40:53,489
It has a heat shield that’s capable
of entering on Earth or Mars.

767
00:40:53,530 --> 00:40:55,282
We can even potentially go to Venus.

768
00:40:55,324 --> 00:40:56,825
but you don't want to go there.

769
00:40:56,867 --> 00:40:58,410
(Laughs)

770
00:40:59,161 --> 00:41:01,371
Venus is hell, almost literally.

771
00:41:02,247 --> 00:41:03,707
But you could ...

772
00:41:04,041 --> 00:41:08,879
It's a generalized method of transport
to anywhere in the solar system,

773
00:41:08,921 --> 00:41:11,757
because the point at which
you have propellant depo on Mars,

774
00:41:11,798 --> 00:41:13,717
you can then travel to the asteroid belt

775
00:41:13,759 --> 00:41:16,887
and to the moons of Jupiter and Saturn

776
00:41:16,887 --> 00:41:19,806
and ultimately anywhere
in the solar system.

777
00:41:19,848 --> 00:41:22,601
CA: But your main focus

778
00:41:22,643 --> 00:41:26,313
and SpaceX's main focus is still Mars.

779
00:41:26,313 --> 00:41:28,524
That is the mission.

780
00:41:28,524 --> 00:41:32,444
That is where most of the effort will go?

781
00:41:33,278 --> 00:41:37,950
Or are you actually imagining
a much broader array of uses

782
00:41:37,991 --> 00:41:40,619
even in the coming, you know,

783
00:41:40,619 --> 00:41:43,872
the first decade or so of uses of this.

784
00:41:44,498 --> 00:41:46,750
Where we could go,
for example, to other places

785
00:41:46,750 --> 00:41:48,669
in the solar system to explore,

786
00:41:48,710 --> 00:41:53,048
perhaps NASA wants to use
the rocket for that reason.

787
00:41:53,423 --> 00:41:58,554
EM: Yeah, NASA is planning to use
a Starship to return to the moon,

788
00:41:58,595 --> 00:42:00,389
to return people to the moon.

789
00:42:01,139 --> 00:42:05,561
And so we're very honored that NASA
has chosen us to do this.

790
00:42:07,271 --> 00:42:11,608
But I'm saying it is a generalized --

791
00:42:11,650 --> 00:42:14,027
it’s a general solution

792
00:42:14,027 --> 00:42:19,199
to getting anywhere
in the greater solar system.

793
00:42:19,241 --> 00:42:21,743
It's not suitable for going
to another star system,

794
00:42:21,785 --> 00:42:25,664
but it is a general solution for transport
anywhere in the solar system.

795
00:42:25,706 --> 00:42:27,291
CA: Before it can do any of that,

796
00:42:27,332 --> 00:42:30,627
it's got to demonstrate it can get into
orbit, you know, around Earth.

797
00:42:30,627 --> 00:42:35,632
What’s your latest advice
on the timeline for that?

798
00:42:35,632 --> 00:42:39,553
EM: It's looking promising for us
to have an orbital launch attempt

799
00:42:39,595 --> 00:42:41,597
in a few months.

800
00:42:43,015 --> 00:42:46,560
So we're actually integrating --

801
00:42:46,602 --> 00:42:49,563
will be integrating the engines
into the booster

802
00:42:49,605 --> 00:42:53,191
for the first orbital flight
starting in about a week or two.

803
00:42:53,942 --> 00:43:00,407
And the launch complex
itself is ready to go.

804
00:43:00,741 --> 00:43:04,411
So assuming we get regulatory approval,

805
00:43:04,453 --> 00:43:10,917
I think we could have an orbital
launch attempt within a few months.

806
00:43:10,959 --> 00:43:12,961
CA: And a radical new technology like this

807
00:43:13,003 --> 00:43:15,547
presumably there is real risk
on those early attempts.

808
00:43:15,589 --> 00:43:16,840
EM: Oh, 100 percent, yeah.

809
00:43:16,882 --> 00:43:20,719
The joke I make all the time
is that excitement is guaranteed.

810
00:43:20,719 --> 00:43:23,513
Success is not guaranteed,
but excitement certainly is.

811
00:43:23,513 --> 00:43:25,891
CA: But the last I saw on your timeline,

812
00:43:25,932 --> 00:43:28,894
you've slightly put back the expected date

813
00:43:28,935 --> 00:43:33,315
to put the first human on Mars
till 2029, I want to say?

814
00:43:33,815 --> 00:43:36,943
EM: Yeah, I mean, so let's see.

815
00:43:36,985 --> 00:43:40,489
I mean, we have built a production
system for Starship,

816
00:43:40,489 --> 00:43:43,784
so we're making a lot
of ships and boosters.

817
00:43:43,784 --> 00:43:45,994
CA: How many are you planning
to make actually?

818
00:43:46,036 --> 00:43:51,750
EM: Well, we're currently expecting
to make a booster and a ship

819
00:43:51,792 --> 00:43:55,295
roughly every, well, initially,
roughly every couple of months,

820
00:43:55,295 --> 00:43:58,965
and then hopefully by the end
of this year, one every month.

821
00:43:59,007 --> 00:44:01,718
So it's giant rockets, and a lot of them.

822
00:44:01,760 --> 00:44:04,179
Just talking in terms
of rough orders of magnitude,

823
00:44:04,179 --> 00:44:07,683
in order to create
a self-sustaining city on Mars,

824
00:44:07,724 --> 00:44:12,145
I think you will need something
on the order of a thousand ships.

825
00:44:12,187 --> 00:44:18,568
And we just need a Helen of Sparta,
I guess, on Mars.

826
00:44:19,319 --> 00:44:21,530
CA: This is not in most
people's heads, Elon.

827
00:44:21,571 --> 00:44:23,532
EM: The planet that launched 1,000 ships.

828
00:44:24,574 --> 00:44:25,742
CA: That's nice.

829
00:44:25,784 --> 00:44:27,661
But this is not in most people's heads,

830
00:44:27,661 --> 00:44:29,579
this picture that you have in your mind.

831
00:44:29,621 --> 00:44:31,373
There's basically a two-year window,

832
00:44:31,373 --> 00:44:34,292
you can only really fly to Mars
conveniently every two years.

833
00:44:34,292 --> 00:44:39,089
You were picturing that during the 2030s,

834
00:44:39,089 --> 00:44:40,465
every couple of years,

835
00:44:40,507 --> 00:44:43,510
something like 1,000 Starships take off,

836
00:44:43,552 --> 00:44:45,512
each containing 100 or more people.

837
00:44:45,512 --> 00:44:50,976
That picture is just completely
mind-blowing to me.

838
00:44:51,393 --> 00:44:54,771
That sense of this armada
of humans going to --

839
00:44:54,813 --> 00:44:57,691
EM: It'll be like "Battlestar
Galactica," the fleet departs.

840
00:44:57,691 --> 00:45:00,485
CA: And you think that it can
basically be funded by people

841
00:45:00,485 --> 00:45:03,739
spending maybe a couple hundred grand
on a ticket to Mars?

842
00:45:03,739 --> 00:45:06,491
Is that price about where it has been?

843
00:45:07,367 --> 00:45:08,994
EM: Well, I think if you say like,

844
00:45:08,994 --> 00:45:13,749
what's required in order to get
enough people and enough cargo to Mars

845
00:45:13,790 --> 00:45:16,376
to build a self-sustaining city.

846
00:45:17,377 --> 00:45:19,296
And it's where you have an intersection

847
00:45:19,296 --> 00:45:21,965
of sets of people who want to go,

848
00:45:21,965 --> 00:45:27,012
because I think only a small percentage
of humanity will want to go,

849
00:45:27,012 --> 00:45:30,766
and can afford to go
or get sponsorship in some manner.

850
00:45:31,391 --> 00:45:33,101
That intersection of sets, I think,

851
00:45:33,101 --> 00:45:35,562
needs to be a million people
or something like that.

852
00:45:36,646 --> 00:45:40,400
And so it’s what can a million people
afford, or get sponsorship for,

853
00:45:40,442 --> 00:45:42,819
because I think governments
will also pay for it,

854
00:45:42,819 --> 00:45:45,822
and people can take out loans.

855
00:45:45,864 --> 00:45:49,576
But I think at the point
at which you say, OK, like,

856
00:45:49,618 --> 00:45:55,999
if moving to Mars costs are,
for argument’s sake, $100,000,

857
00:45:56,041 --> 00:46:01,213
then I think you know,
almost anyone can work and save up

858
00:46:01,213 --> 00:46:05,258
and eventually have $100,000
and be able to go to Mars if they want.

859
00:46:05,300 --> 00:46:07,969
We want to make it available
to anyone who wants to go.

860
00:46:10,263 --> 00:46:14,434
It's very important to emphasize
that Mars, especially in the beginning,

861
00:46:14,434 --> 00:46:15,727
will not be luxurious.

862
00:46:15,727 --> 00:46:21,399
It will be dangerous, cramped,
difficult, hard work.

863
00:46:22,025 --> 00:46:25,195
It's kind of like that Shackleton ad
for going to the Antarctic,

864
00:46:25,237 --> 00:46:28,573
which I think is actually not real,
but it sounds real and it's cool.

865
00:46:28,865 --> 00:46:31,743
It's sort of like, the sales pitch
for going to Mars is,

866
00:46:31,785 --> 00:46:34,663
"It's dangerous, it's cramped.

867
00:46:35,956 --> 00:46:37,457
You might not make it back.

868
00:46:38,750 --> 00:46:40,293
It's difficult, it's hard work."

869
00:46:40,293 --> 00:46:41,461
That's the sales pitch.

870
00:46:41,503 --> 00:46:42,671
CA: Right.

871
00:46:42,712 --> 00:46:43,964
But you will make history.

872
00:46:44,756 --> 00:46:46,341
EM: But it'll be glorious.

873
00:46:47,050 --> 00:46:50,512
CA: So on that kind of launch rate
you're talking about over two decades,

874
00:46:50,554 --> 00:46:54,182
you could get your million people
to Mars, essentially.

875
00:46:54,224 --> 00:46:55,350
Whose city is it?

876
00:46:55,392 --> 00:46:57,310
Is it NASA's city, is it SpaceX's city?

877
00:46:57,352 --> 00:46:58,979
EM: It’s the people of Mars’ city.

878
00:47:01,106 --> 00:47:05,735
The reason for this, I mean,
I feel like why do this thing?

879
00:47:05,735 --> 00:47:10,073
I think this is important for maximizing

880
00:47:10,115 --> 00:47:13,159
the probable lifespan of humanity
or consciousness.

881
00:47:13,201 --> 00:47:17,205
Human civilization could come
to an end for external reasons,

882
00:47:17,247 --> 00:47:22,419
like a giant meteor or super volcanoes
or extreme climate change.

883
00:47:24,045 --> 00:47:29,801
Or World War III, or you know,
any one of a number of reasons.

884
00:47:32,929 --> 00:47:35,807
But the probable life span
of civilizational consciousness

885
00:47:35,849 --> 00:47:37,434
as we know it,

886
00:47:37,475 --> 00:47:40,604
which we should really view
as this very delicate thing,

887
00:47:40,645 --> 00:47:43,356
like a small candle in a vast darkness.

888
00:47:43,356 --> 00:47:46,318
That is what appears to be the case.

889
00:47:47,777 --> 00:47:51,156
We're in this vast darkness of space,

890
00:47:51,197 --> 00:47:54,618
and there's this little
candle of consciousness

891
00:47:54,659 --> 00:47:59,122
that’s only really come about
after 4.5 billion years,

892
00:47:59,164 --> 00:48:01,166
and it could just go out.

893
00:48:01,166 --> 00:48:02,542
CA: I think that's powerful,

894
00:48:02,584 --> 00:48:05,420
and I think a lot of people
will be inspired by that vision.

895
00:48:05,420 --> 00:48:07,547
And the reason you need the million people

896
00:48:07,547 --> 00:48:09,716
is because there has to be
enough people there

897
00:48:09,758 --> 00:48:12,177
to do everything that you need to survive.

898
00:48:13,136 --> 00:48:19,976
EM: Really, like the critical threshold
is if the ships from Earth stop coming

899
00:48:20,018 --> 00:48:22,562
for any reason,

900
00:48:22,604 --> 00:48:26,983
does the Mars City die out or not?

901
00:48:27,400 --> 00:48:29,486
And so we have to --

902
00:48:29,527 --> 00:48:32,656
You know, people talk about like,
the sort of, the great filters,

903
00:48:32,697 --> 00:48:35,784
the things that perhaps, you know,

904
00:48:35,825 --> 00:48:38,536
we talk about the Fermi paradox,
and where are the aliens?

905
00:48:38,536 --> 00:48:40,830
Well maybe there are these
various great filters

906
00:48:40,872 --> 00:48:42,290
that the aliens didn’t pass,

907
00:48:42,290 --> 00:48:46,878
and so they eventually
just ceased to exist.

908
00:48:46,920 --> 00:48:50,048
And one of the great filters
is becoming a multi-planet species.

909
00:48:50,674 --> 00:48:52,884
So we want to pass that filter.

910
00:48:54,302 --> 00:49:00,308
And I'll be long-dead before
this is, you know, a real thing,

911
00:49:00,350 --> 00:49:01,601
before it happens.

912
00:49:01,601 --> 00:49:06,898
But I’d like to at least see us make
great progress in this direction.

913
00:49:07,315 --> 00:49:09,818
CA: Given how tortured
the Earth is right now,

914
00:49:09,859 --> 00:49:12,737
how much we're beating each other up,

915
00:49:12,737 --> 00:49:15,532
shouldn't there be discussions going on

916
00:49:15,573 --> 00:49:19,744
with everyone who is dreaming
about Mars to try to say,

917
00:49:19,786 --> 00:49:24,958
we've got a once
in a civilization's chance

918
00:49:24,958 --> 00:49:26,960
to make some new rules here?

919
00:49:27,002 --> 00:49:30,755
Should someone be trying
to lead those discussions

920
00:49:30,755 --> 00:49:34,634
to figure out what it means for this
to be the people of Mars' City?

921
00:49:35,093 --> 00:49:36,469
EM: Well, I think ultimately

922
00:49:36,469 --> 00:49:38,680
this will be up to the people
of Mars to decide

923
00:49:38,722 --> 00:49:42,767
how they want to rethink society.

924
00:49:43,101 --> 00:49:44,728
Yeah there’s certainly risk there.

925
00:49:45,395 --> 00:49:50,025
And hopefully the people of Mars
will be more enlightened

926
00:49:50,066 --> 00:49:52,777
and will not fight
amongst each other too much.

927
00:49:54,279 --> 00:49:56,031
I mean, I have some recommendations,

928
00:49:56,031 --> 00:49:59,993
which people of Mars
may choose to listen to or not.

929
00:50:00,035 --> 00:50:02,829
I would advocate for more
of a direct democracy,

930
00:50:02,829 --> 00:50:05,040
not a representative democracy,

931
00:50:05,040 --> 00:50:07,751
and laws that are short enough
for people to understand.

932
00:50:08,168 --> 00:50:13,548
Where it is harder to create laws
than to get rid of them.

933
00:50:14,424 --> 00:50:16,092
CA: Coming back a bit nearer term,

934
00:50:16,134 --> 00:50:19,596
I'd love you to just talk a bit
about some of the other possibility space

935
00:50:19,596 --> 00:50:23,308
that Starship seems to have created.

936
00:50:23,349 --> 00:50:24,726
So given --

937
00:50:24,768 --> 00:50:28,271
Suddenly we've got this ability
to move 100 tons-plus into orbit.

938
00:50:29,230 --> 00:50:32,400
So we've just launched
the James Webb telescope,

939
00:50:32,400 --> 00:50:34,402
which is an incredible thing.

940
00:50:34,444 --> 00:50:35,570
It's unbelievable.

941
00:50:35,612 --> 00:50:37,405
EM: Exquisite piece of technology.

942
00:50:37,447 --> 00:50:39,074
CA: Exquisite piece of technology.

943
00:50:39,115 --> 00:50:42,619
But people spent two years trying
to figure out how to fold up this thing.

944
00:50:42,660 --> 00:50:43,995
It's a three-ton telescope.

945
00:50:44,037 --> 00:50:47,207
EM: We can make it a lot easier
if you’ve got more volume and mass.

946
00:50:47,207 --> 00:50:49,084
CA: But let's ask a different question.

947
00:50:49,084 --> 00:50:55,840
Which is, how much more powerful
a telescope could someone design

948
00:50:55,882 --> 00:50:58,760
based on using Starship, for example?

949
00:50:59,469 --> 00:51:04,015
EM: I mean, roughly, I'd say it's probably
an order of magnitude more resolution.

950
00:51:04,057 --> 00:51:07,268
If you've got 100 tons
and a thousand cubic meters volume,

951
00:51:07,268 --> 00:51:08,853
which is roughly what we have.

952
00:51:08,895 --> 00:51:12,440
CA: And what about other exploration
through the solar system?

953
00:51:12,482 --> 00:51:14,651
I mean, I'm you know --

954
00:51:14,692 --> 00:51:17,362
EM: Europa is a big question mark.

955
00:51:17,403 --> 00:51:19,197
CA: Right, so there's an ocean there.

956
00:51:19,239 --> 00:51:22,534
And what you really want to do
is to drop a submarine into that ocean.

957
00:51:22,534 --> 00:51:24,828
EM: Maybe there's like,
some squid civilization,

958
00:51:24,869 --> 00:51:28,081
cephalopod civilization
under the ice of Europa.

959
00:51:28,123 --> 00:51:29,749
That would be pretty interesting.

960
00:51:29,749 --> 00:51:32,460
CA: I mean, Elon, if you could take
a submarine to Europa

961
00:51:32,460 --> 00:51:36,089
and we see pictures of this thing
being devoured by a squid,

962
00:51:36,131 --> 00:51:38,675
that would honestly be
the happiest moment of my life.

963
00:51:38,716 --> 00:51:40,093
EM: Pretty wild, yeah.

964
00:51:40,426 --> 00:51:43,221
CA: What other possibilities
are out there?

965
00:51:43,263 --> 00:51:47,642
Like, it feels like if you're going to
create a thousand of these things,

966
00:51:47,642 --> 00:51:50,854
they can only fly to Mars every two years.

967
00:51:50,895 --> 00:51:53,064
What are they doing the rest of the time?

968
00:51:53,106 --> 00:51:57,777
It feels like there's this
explosion of possibility

969
00:51:57,777 --> 00:52:00,238
that I don't think people
are really thinking about.

970
00:52:00,238 --> 00:52:02,866
EM: I don't know, we've certainly
got a long way to go.

971
00:52:02,866 --> 00:52:05,618
As you alluded to earlier,
we still have to get to orbit.

972
00:52:05,618 --> 00:52:07,370
And then after we get to orbit,

973
00:52:07,412 --> 00:52:13,418
we have to really prove out and refine
full and rapid reusability.

974
00:52:14,085 --> 00:52:15,295
That'll take a moment.

975
00:52:19,090 --> 00:52:20,842
But I do think we will solve this.

976
00:52:22,969 --> 00:52:25,680
I'm highly confident
we will solve this at this point.

977
00:52:26,014 --> 00:52:27,807
CA: Do you ever wake up with the fear

978
00:52:27,849 --> 00:52:31,311
that there's going to be this
Hindenburg moment for SpaceX where ...

979
00:52:31,811 --> 00:52:33,271
EM: We've had many Hindenburg.

980
00:52:33,313 --> 00:52:36,983
Well, we've never had Hindenburg moments
with people, which is very important.

981
00:52:37,025 --> 00:52:38,276
Big difference.

982
00:52:38,776 --> 00:52:40,486
We've blown up quite a few rockets.

983
00:52:40,486 --> 00:52:43,990
So there's a whole compilation online
that we put together

984
00:52:44,032 --> 00:52:45,742
and others put together,

985
00:52:45,742 --> 00:52:47,702
it's showing rockets are hard.

986
00:52:47,744 --> 00:52:51,539
I mean, the sheer amount of energy
going through a rocket boggles the mind.

987
00:52:51,581 --> 00:52:55,084
So, you know, getting out
of Earth's gravity well is difficult.

988
00:52:55,126 --> 00:52:57,503
We have a strong gravity
and a thick atmosphere.

989
00:52:59,422 --> 00:53:03,343
And Mars, which is less than 40 percent,

990
00:53:03,426 --> 00:53:06,137
it's like, 37 percent of Earth's gravity

991
00:53:06,179 --> 00:53:07,805
and has a thin atmosphere.

992
00:53:08,097 --> 00:53:10,099
The ship alone can go all the way

993
00:53:10,141 --> 00:53:12,560
from the surface of Mars
to the surface of Earth.

994
00:53:12,602 --> 00:53:17,357
Whereas getting to Mars requires
a giant booster and orbital refilling.

995
00:53:17,774 --> 00:53:22,570
CA: So, Elon, as I think more
about this incredible array of things

996
00:53:22,612 --> 00:53:24,447
that you're involved with,

997
00:53:24,489 --> 00:53:28,785
I keep seeing these synergies,

998
00:53:28,826 --> 00:53:30,703
to use a horrible word,

999
00:53:30,745 --> 00:53:31,913
between them.

1000
00:53:31,955 --> 00:53:33,122
You know, for example,

1001
00:53:33,122 --> 00:53:38,878
the robots you're building from Tesla
could possibly be pretty handy on Mars,

1002
00:53:38,920 --> 00:53:41,089
doing some of the dangerous
work and so forth.

1003
00:53:41,089 --> 00:53:43,758
I mean, maybe there's a scenario
where your city on Mars

1004
00:53:43,758 --> 00:53:45,218
doesn't need a million people,

1005
00:53:45,218 --> 00:53:47,929
it needs half a million people
and half a million robots.

1006
00:53:47,971 --> 00:53:49,806
And that's a possibility.

1007
00:53:49,847 --> 00:53:52,058
Maybe The Boring Company could play a role

1008
00:53:52,100 --> 00:53:57,480
helping create some of the subterranean
dwelling spaces that you might need.

1009
00:53:57,480 --> 00:53:58,690
EM: Yeah.

1010
00:53:58,982 --> 00:54:00,483
CA: Back on planet Earth,

1011
00:54:00,525 --> 00:54:03,736
it seems like a partnership
between Boring Company and Tesla

1012
00:54:03,778 --> 00:54:07,657
could offer an unbelievable deal to a city

1013
00:54:07,699 --> 00:54:12,203
to say, we will create for you
a 3D network of tunnels

1014
00:54:12,245 --> 00:54:14,497
populated by robo-taxis

1015
00:54:14,539 --> 00:54:18,793
that will offer fast, low-cost
transport to anyone.

1016
00:54:18,835 --> 00:54:21,713
You know, full self-driving may
or may not be done this year.

1017
00:54:21,713 --> 00:54:24,507
And in some cities,
like, somewhere like Mumbai,

1018
00:54:24,549 --> 00:54:26,592
I suspect won't be done for a decade.

1019
00:54:26,634 --> 00:54:28,928
EM: Some places are more
challenging than others.

1020
00:54:28,928 --> 00:54:31,306
CA: But today, today,
with what you've got,

1021
00:54:31,306 --> 00:54:35,560
you could put a 3D network
of tunnels under there.

1022
00:54:35,601 --> 00:54:38,396
EM: Oh, if it’s just in a tunnel,
that’s a solved problem.

1023
00:54:38,438 --> 00:54:40,940
CA: Exactly, full self-driving
is a solved problem.

1024
00:54:40,982 --> 00:54:44,027
To me, there’s amazing synergy there.

1025
00:54:44,068 --> 00:54:45,820
With Starship,

1026
00:54:45,820 --> 00:54:51,159
you know, Gwynne Shotwell talked
about by 2028 having from city to city,

1027
00:54:51,200 --> 00:54:52,952
you know, transport on planet Earth.

1028
00:54:52,952 --> 00:54:54,579
EM: This is a real possibility.

1029
00:54:57,290 --> 00:55:00,251
The fastest way to get
from one place to another,

1030
00:55:00,293 --> 00:55:02,086
if it's a long distance, is a rocket.

1031
00:55:03,087 --> 00:55:04,464
It's basically an ICBM.

1032
00:55:05,673 --> 00:55:07,008
CA: But it has to land --

1033
00:55:07,675 --> 00:55:11,554
Because it's an ICBM,
it has to land probably offshore,

1034
00:55:11,596 --> 00:55:12,764
because it's loud.

1035
00:55:12,805 --> 00:55:19,437
So why not have a tunnel
that then connects to the city with Tesla?

1036
00:55:20,897 --> 00:55:22,023
And Neuralink.

1037
00:55:22,065 --> 00:55:23,691
I mean, if you going to go to Mars

1038
00:55:23,733 --> 00:55:26,611
having a telepathic connection
with loved ones back home,

1039
00:55:26,611 --> 00:55:28,112
even if there's a time delay...

1040
00:55:29,238 --> 00:55:33,326
EM: These are not intended
to be connected, by the way.

1041
00:55:33,326 --> 00:55:35,745
But there certainly could be
some synergies, yeah.

1042
00:55:35,787 --> 00:55:38,081
CA: Surely there is a growing argument

1043
00:55:38,081 --> 00:55:40,792
that you should actually put
all these things together

1044
00:55:40,792 --> 00:55:42,919
into one company

1045
00:55:42,960 --> 00:55:47,256
and just have a company
devoted to creating a future

1046
00:55:47,298 --> 00:55:48,758
that’s exciting,

1047
00:55:48,758 --> 00:55:50,385
and let a thousand flowers bloom.

1048
00:55:50,426 --> 00:55:52,053
Have you been thinking about that?

1049
00:55:53,429 --> 00:55:56,682
EM: I mean, it is tricky because Tesla
is a publicly-traded company,

1050
00:55:56,724 --> 00:56:01,979
and the investor base of Tesla and SpaceX

1051
00:56:02,021 --> 00:56:05,400
and certainly Boring Company
and Neuralink are quite different.

1052
00:56:05,441 --> 00:56:07,944
Boring Company and Neuralink
are tiny companies.

1053
00:56:08,569 --> 00:56:09,987
CA: By comparison.

1054
00:56:10,321 --> 00:56:13,950
EM: Yeah, Tesla's got 110,000 people.

1055
00:56:14,283 --> 00:56:16,786
SpaceX I think is around 12,000 people.

1056
00:56:17,161 --> 00:56:21,707
Boring Company and Neuralink
are both under 200 people.

1057
00:56:21,749 --> 00:56:24,919
So they're little, tiny companies,

1058
00:56:24,961 --> 00:56:27,213
but they will probably
get bigger in the future.

1059
00:56:27,213 --> 00:56:28,923
They will get bigger in the future.

1060
00:56:29,632 --> 00:56:32,176
It's not that easy to sort
of combine these things.

1061
00:56:33,761 --> 00:56:36,639
CA: Traditionally, you have said
that for SpaceX especially,

1062
00:56:36,639 --> 00:56:38,057
you wouldn't want it public,

1063
00:56:38,057 --> 00:56:42,353
because public investors wouldn't support
the craziness of the idea

1064
00:56:42,395 --> 00:56:43,813
of going to Mars or whatever.

1065
00:56:43,813 --> 00:56:45,857
EM: Yeah, making life multi-planetary

1066
00:56:45,898 --> 00:56:51,779
is outside of the normal time horizon
of Wall Street analysts.

1067
00:56:51,779 --> 00:56:52,780
(Laughs)

1068
00:56:52,864 --> 00:56:54,073
To say the least.

1069
00:56:54,073 --> 00:56:56,659
CA: I think something's changed, though.

1070
00:56:56,701 --> 00:56:59,454
What's changed is that Tesla is now
so powerful and so big

1071
00:56:59,495 --> 00:57:01,956
and throws off so much cash

1072
00:57:01,998 --> 00:57:05,501
that you actually could
connect the dots here.

1073
00:57:05,501 --> 00:57:10,047
Just tell the public that x-billion
dollars a year, whatever your number is,

1074
00:57:10,089 --> 00:57:13,509
will be diverted to the Mars mission.

1075
00:57:13,551 --> 00:57:17,013
I suspect you'd have massive
interest in that company.

1076
00:57:17,054 --> 00:57:21,976
And it might unlock a lot
more possibility for you, no?

1077
00:57:22,018 --> 00:57:27,815
EM: I would like to give the public access
to ownership of SpaceX,

1078
00:57:27,815 --> 00:57:30,526
but I mean the thing that like,

1079
00:57:30,568 --> 00:57:35,698
the overhead associated
with a public company is high.

1080
00:57:38,284 --> 00:57:40,995
I mean, as a public company,
you're just constantly sued.

1081
00:57:41,037 --> 00:57:44,040
It does occupy like, a fair bit of ...

1082
00:57:45,958 --> 00:57:49,504
You know, time and effort
to deal with these things.

1083
00:57:49,504 --> 00:57:53,132
CA: But you would still only have one
public company, it would be bigger,

1084
00:57:53,174 --> 00:57:54,926
and have more things going on.

1085
00:57:54,967 --> 00:57:57,678
But instead of being
on four boards, you'd be on one.

1086
00:57:57,720 --> 00:58:01,057
EM: I'm actually not even on the Neuralink
or Boring Company boards.

1087
00:58:02,099 --> 00:58:05,770
And I don't really attend
the SpaceX board meetings.

1088
00:58:06,103 --> 00:58:07,313
We only have two a year,

1089
00:58:07,313 --> 00:58:09,524
and I just stop by and chat for an hour.

1090
00:58:13,110 --> 00:58:15,947
The board overhead for a public
company is much higher.

1091
00:58:15,988 --> 00:58:19,700
CA: I think some investors probably worry
about how your time is being split,

1092
00:58:19,742 --> 00:58:22,411
and they might be excited
by you know, that.

1093
00:58:22,495 --> 00:58:25,748
Anyway, I just woke up the other day

1094
00:58:25,790 --> 00:58:29,210
thinking, just, there are so many ways
in which these things connect.

1095
00:58:29,252 --> 00:58:33,089
And you know,
just the simplicity of that mission,

1096
00:58:33,130 --> 00:58:36,676
of building a future that is worth
getting excited about,

1097
00:58:36,676 --> 00:58:40,137
might appeal to an awful lot of people.

1098
00:58:41,013 --> 00:58:46,394
Elon, you are reported by Forbes
and everyone else as now, you know,

1099
00:58:46,435 --> 00:58:48,187
the world's richest person.

1100
00:58:48,187 --> 00:58:49,480
EM: That’s not a sovereign.

1101
00:58:49,564 --> 00:58:50,565
CA: (Laughs)

1102
00:58:50,606 --> 00:58:52,441
EM: You know, I think it’s fair to say

1103
00:58:52,483 --> 00:58:57,154
that if somebody is like, the king
or de facto king of a country,

1104
00:58:57,154 --> 00:58:59,115
they're wealthier than I am.

1105
00:58:59,323 --> 00:59:02,034
CA: But it’s just harder to measure --

1106
00:59:02,285 --> 00:59:03,703
So $300 billion.

1107
00:59:03,744 --> 00:59:07,582
I mean, your net worth on any given day

1108
00:59:07,623 --> 00:59:10,668
is rising or falling
by several billion dollars.

1109
00:59:10,710 --> 00:59:12,837
How insane is that?

1110
00:59:12,878 --> 00:59:14,046
EM: It's bonkers, yeah.

1111
00:59:14,088 --> 00:59:16,674
CA: I mean, how do you handle
that psychologically?

1112
00:59:16,674 --> 00:59:20,177
There aren't many people in the world
who have to even think about that.

1113
00:59:20,177 --> 00:59:22,388
EM: I actually don't think
about that too much.

1114
00:59:22,430 --> 00:59:26,017
But the thing that is
actually more difficult

1115
00:59:26,058 --> 00:59:27,935
and that does make sleeping difficult

1116
00:59:27,977 --> 00:59:31,355
is that, you know,

1117
00:59:31,397 --> 00:59:34,984
every good hour or even minute

1118
00:59:35,026 --> 00:59:39,447
of thinking about Tesla and SpaceX

1119
00:59:39,447 --> 00:59:41,949
has such a big effect on the company

1120
00:59:41,991 --> 00:59:45,911
that I really try to work
as much as possible,

1121
00:59:45,911 --> 00:59:49,040
you know, to the edge
of sanity, basically.

1122
00:59:49,081 --> 00:59:52,418
Because you know,
Tesla’s getting to the point where

1123
00:59:54,920 --> 00:59:57,131
probably will get
to the point later this year,

1124
00:59:57,131 --> 01:00:02,178
where every high-quality
minute of thinking

1125
01:00:02,219 --> 01:00:05,890
is a million dollars impact on Tesla.

1126
01:00:08,517 --> 01:00:10,061
Which is insane.

1127
01:00:13,272 --> 01:00:17,318
I mean, the basic, you know,
if Tesla is doing, you know,

1128
01:00:17,360 --> 01:00:21,280
sort of $2 billion a week,
let’s say, in revenue,

1129
01:00:21,280 --> 01:00:25,993
it’s sort of $300 million a day,
seven days a week.

1130
01:00:26,535 --> 01:00:27,870
You know, it's ...

1131
01:00:28,829 --> 01:00:34,377
CA: If you can change that by five percent
in an hour’s brainstorm,

1132
01:00:34,418 --> 01:00:37,546
that's a pretty valuable hour.

1133
01:00:37,546 --> 01:00:42,343
EM: I mean, there are many instances
where a half-hour meeting,

1134
01:00:42,385 --> 01:00:45,763
I was able to improve
the financial outcome of the company

1135
01:00:45,763 --> 01:00:49,392
by $100 million
in a half-hour meeting.

1136
01:00:50,476 --> 01:00:52,520
CA: There are many other people out there

1137
01:00:52,520 --> 01:00:55,272
who can't stand
this world of billionaires.

1138
01:00:55,314 --> 01:00:59,235
Like, they are hugely
offended by the notion

1139
01:00:59,276 --> 01:01:03,864
that an individual can have
the same wealth as, say,

1140
01:01:03,906 --> 01:01:07,118
a billion or more
of the world's poorest people.

1141
01:01:07,159 --> 01:01:09,578
EM: If they examine sort of --

1142
01:01:09,578 --> 01:01:14,625
I think there's some axiomatic flaws
that are leading them to that conclusion.

1143
01:01:15,167 --> 01:01:20,089
For sure, it would be very
problematic if I was consuming,

1144
01:01:20,131 --> 01:01:23,217
you know, billions of dollars a year
in personal consumption.

1145
01:01:23,259 --> 01:01:24,468
But that is not the case.

1146
01:01:24,802 --> 01:01:27,054
In fact, I don't even own
a home right now.

1147
01:01:27,096 --> 01:01:29,432
I'm literally staying at friends' places.

1148
01:01:30,141 --> 01:01:31,684
If I travel to the Bay Area,

1149
01:01:31,726 --> 01:01:33,811
which is where most
of Tesla engineering is,

1150
01:01:33,811 --> 01:01:37,606
I basically rotate through
friends' spare bedrooms.

1151
01:01:38,691 --> 01:01:41,444
I don't have a yacht,
I really don't take vacations.

1152
01:01:44,071 --> 01:01:48,409
It’s not as though my personal
consumption is high.

1153
01:01:49,243 --> 01:01:51,036
I mean, the one exception is a plane.

1154
01:01:51,078 --> 01:01:53,956
But if I don't use the plane,
then I have less hours to work.

1155
01:01:55,291 --> 01:01:59,420
CA: I mean, I personally think
you have shown that you are mostly driven

1156
01:01:59,420 --> 01:02:01,922
by really quite a deep
sense of moral purpose.

1157
01:02:01,964 --> 01:02:07,553
Like, your attempts to solve
the climate problem

1158
01:02:07,595 --> 01:02:12,433
have been as powerful as anyone else
on the planet that I'm aware of.

1159
01:02:12,433 --> 01:02:14,518
And I actually can't understand,

1160
01:02:14,518 --> 01:02:16,395
personally, I can't understand the fact

1161
01:02:16,437 --> 01:02:18,898
that you get all this criticism
from the Left about,

1162
01:02:18,898 --> 01:02:21,275
"Oh, my God, he's so rich,
that's disgusting."

1163
01:02:21,734 --> 01:02:24,111
When climate is their issue.

1164
01:02:25,446 --> 01:02:27,656
Philanthropy is a topic
that some people go to.

1165
01:02:27,698 --> 01:02:29,366
Philanthropy is a hard topic.

1166
01:02:29,408 --> 01:02:31,202
How do you think about that?

1167
01:02:31,535 --> 01:02:34,246
EM: I think if you care
about the reality of goodness

1168
01:02:34,246 --> 01:02:38,042
instead of the perception of it,
philanthropy is extremely difficult.

1169
01:02:39,126 --> 01:02:43,047
SpaceX, Tesla, Neuralink
and The Boring Company are philanthropy.

1170
01:02:43,464 --> 01:02:46,550
If you say philanthropy
is love of humanity,

1171
01:02:46,592 --> 01:02:48,260
they are philanthropy.

1172
01:02:49,553 --> 01:02:52,431
Tesla is accelerating sustainable energy.

1173
01:02:52,473 --> 01:02:56,018
This is a love -- philanthropy.

1174
01:02:56,894 --> 01:03:00,606
SpaceX is trying to ensure
the long-term survival of humanity

1175
01:03:00,648 --> 01:03:02,149
with a multiple-planet species.

1176
01:03:02,191 --> 01:03:03,734
That is love of humanity.

1177
01:03:05,319 --> 01:03:09,865
You know, Neuralink is trying to help
solve brain injuries

1178
01:03:09,907 --> 01:03:12,201
and existential risk with AI.

1179
01:03:12,243 --> 01:03:13,410
Love of humanity.

1180
01:03:13,452 --> 01:03:16,956
Boring Company is trying to solve traffic,
which is hell for most people,

1181
01:03:16,956 --> 01:03:19,583
and that also is love of humanity.

1182
01:03:20,084 --> 01:03:24,380
CA: How upsetting is it to you

1183
01:03:24,421 --> 01:03:27,967
to hear this constant drumbeat of,

1184
01:03:28,008 --> 01:03:30,177
"Billionaires, my God,
Elon Musk, oh, my God?"

1185
01:03:30,219 --> 01:03:33,180
Like, do you just shrug that off

1186
01:03:33,222 --> 01:03:34,849
or does it does it actually hurt?

1187
01:03:36,559 --> 01:03:39,353
EM: I mean, at this point,
it's water off a duck's back.

1188
01:03:39,353 --> 01:03:41,897
CA: Elon, I’d like to,
as we wrap up now,

1189
01:03:41,939 --> 01:03:45,317
just pull the camera back
and just think ...

1190
01:03:45,359 --> 01:03:48,863
You’re a father now
of seven surviving kids.

1191
01:03:49,530 --> 01:03:51,866
EM: Well, I mean, I'm trying
to set a good example

1192
01:03:51,907 --> 01:03:53,826
because the birthrate on Earth is so low

1193
01:03:53,868 --> 01:03:55,911
that we're facing civilizational collapse

1194
01:03:55,911 --> 01:04:00,749
unless the birth rate returns
to a sustainable level.

1195
01:04:01,667 --> 01:04:03,627
CA: Yeah, you've talked about this a lot,

1196
01:04:03,669 --> 01:04:05,963
that depopulation is a big problem,

1197
01:04:06,005 --> 01:04:08,465
and people don't understand
how big a problem it is.

1198
01:04:08,465 --> 01:04:10,968
EM: Population collapse
is one of the biggest threats

1199
01:04:10,968 --> 01:04:12,720
to the future of human civilization.

1200
01:04:12,761 --> 01:04:14,638
And that is what is going on right now.

1201
01:04:14,638 --> 01:04:17,474
CA: What drives you on a day-to-day
basis to do what you do?

1202
01:04:17,516 --> 01:04:20,603
EM: I guess, like,
I really want to make sure

1203
01:04:20,644 --> 01:04:24,106
that there is a good future for humanity

1204
01:04:24,148 --> 01:04:29,445
and that we're on a path to understanding
the nature of the universe,

1205
01:04:29,486 --> 01:04:30,654
the meaning of life.

1206
01:04:30,696 --> 01:04:32,656
Why are we here, how did we get here?

1207
01:04:33,490 --> 01:04:37,912
And in order to understand
the nature of the universe

1208
01:04:37,953 --> 01:04:41,874
and all these fundamental questions,

1209
01:04:41,916 --> 01:04:45,002
we must expand the scope
and scale of consciousness.

1210
01:04:47,129 --> 01:04:49,089
Certainly it must not diminish or go out.

1211
01:04:49,131 --> 01:04:51,342
Or we certainly won’t understand this.

1212
01:04:51,342 --> 01:04:54,929
I would say I’ve been motivated
by curiosity more than anything,

1213
01:04:54,929 --> 01:04:59,266
and just desire to think about the future

1214
01:04:59,308 --> 01:05:01,852
and not be sad, you know?

1215
01:05:03,687 --> 01:05:04,855
CA: And are you?

1216
01:05:04,897 --> 01:05:06,148
Are you not sad?

1217
01:05:06,607 --> 01:05:07,816
EM: I'm sometimes sad,

1218
01:05:07,816 --> 01:05:12,321
but mostly I'm feeling I guess

1219
01:05:12,363 --> 01:05:14,907
relatively optimistic
about the future these days.

1220
01:05:15,699 --> 01:05:19,620
There are certainly some big
risks that humanity faces.

1221
01:05:20,287 --> 01:05:23,082
I think the population collapse
is a really big deal,

1222
01:05:23,123 --> 01:05:28,253
that I wish more people would think about

1223
01:05:28,253 --> 01:05:33,217
because the birth rate is far below
what's needed to sustain civilization

1224
01:05:33,258 --> 01:05:34,927
at its current level.

1225
01:05:35,594 --> 01:05:38,806
And there's obviously ...

1226
01:05:39,682 --> 01:05:42,559
We need to take action
on climate sustainability,

1227
01:05:42,601 --> 01:05:44,520
which is being done.

1228
01:05:45,562 --> 01:05:47,856
And we need to secure
the future of consciousness

1229
01:05:47,898 --> 01:05:50,150
by being a multi-planet species.

1230
01:05:51,151 --> 01:05:52,444
We need to address --

1231
01:05:52,486 --> 01:05:55,698
Essentially, it's important to take
whatever actions we can think of

1232
01:05:55,698 --> 01:06:00,494
to address the existential risks
that affect the future of consciousness.

1233
01:06:00,536 --> 01:06:02,663
CA: There's a whole
generation coming through

1234
01:06:02,663 --> 01:06:04,456
who seem really sad about the future.

1235
01:06:04,456 --> 01:06:06,250
What would you say to them?

1236
01:06:07,376 --> 01:06:10,963
EM: Well, I think if you want the future
to be good, you must make it so.

1237
01:06:12,256 --> 01:06:14,675
Take action to make it good.

1238
01:06:14,717 --> 01:06:15,926
And it will be.

1239
01:06:17,177 --> 01:06:19,388
CA: Elon, thank you for all this time.

1240
01:06:19,722 --> 01:06:21,390
That is a beautiful place to end.

1241
01:06:21,390 --> 01:06:22,766
Thanks for all you're doing.

1242
01:06:22,766 --> 01:06:23,976
EM: You're welcome.
